{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, psutil\n",
    "\n",
    "import netket as nk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from tqdm.notebook import tqdm as tq\n",
    "import itertools\n",
    "from functools import reduce\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy versions\n",
    "matmap_np = {\n",
    "    \"H\": np.array([[1, 1], [1, -1]], dtype=float) / np.sqrt(2),\n",
    "    \"X\": np.array([[0, 1], [1, 0]], dtype=float),\n",
    "    # \"Y\": np.array([[0, -1j], [1j, 0]], dtype=complex), # not needed for Ising models\n",
    "    \"Z\": np.array([[1, 0], [0, -1]], dtype=float),\n",
    "    \"I\": np.eye(2**1, dtype=float)\n",
    "}\n",
    "# some pre-computed matrices that might be useful for Ising models\n",
    "matmap_np.update({\n",
    "    \"II\": np.eye(2**2, dtype=float), # this line alone would already do most of the work\n",
    "    \"ZZ\": np.kron(matmap_np[\"Z\"], matmap_np[\"Z\"]),\n",
    "    \"IX\": np.kron(matmap_np[\"I\"], matmap_np[\"X\"]),\n",
    "    \"XI\": np.kron(matmap_np[\"X\"], matmap_np[\"I\"]),\n",
    "    \"III\": np.eye(2**3, dtype=float),\n",
    "    \"IIII\": np.eye(2**4, dtype=float),\n",
    "    \"IIIII\": np.eye(2**5, dtype=float),\n",
    "    \"IIIIII\": np.eye(2**6, dtype=float),\n",
    "    \"IIIIIII\": np.eye(2**7, dtype=float),\n",
    "    \"IIIIIIII\": np.eye(2**8, dtype=float),\n",
    "    \"IIIIIIIII\": np.eye(2**9, dtype=float),\n",
    "    \"IIIIIIIIII\": np.eye(2**10, dtype=float),\n",
    "})\n",
    "\n",
    "# sparse versions\n",
    "matmap_sp = {k: sp.csr_array(v) for k, v in matmap_np.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapp(func, *iterables, cb=None, **kwargs):\n",
    "    \"\"\"map function that uses tq for progress bar\"\"\"\n",
    "    for i in tq(range(len(iterables[0]))):\n",
    "        res = func(*[iterable[i] for iterable in iterables], **kwargs)\n",
    "        if cb is not None:\n",
    "            cb(res)\n",
    "        yield res\n",
    "\n",
    "def imshow(a, figsize=(8,6), cmap='seismic', **pltargs):\n",
    "    \"\"\"Uses magic to create pretty images from arrays.\"\"\"\n",
    "\n",
    "    if sp.issparse(a):\n",
    "        a = a.todense()\n",
    "    else:\n",
    "        a = np.array(a, copy=False)\n",
    "    if np.prod(a.shape) == np.max(a.shape):\n",
    "        a = a.flatten()\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    cmap = plt.get_cmap(cmap)\n",
    "    if 'vmin' not in pltargs and 'vmax' not in pltargs:\n",
    "        vmax = np.max(np.abs(a))\n",
    "        vmin = -vmax\n",
    "    plt.imshow(a, cmap=cmap, vmin=vmin, vmax=vmax, **pltargs)\n",
    "    plt.colorbar()\n",
    "\n",
    "def hist(data, bins=None, xlabel=\"\", title=\"\", density=False):\n",
    "    def bins_sqrt(data):\n",
    "        return int(np.ceil(np.sqrt(len(data))))\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    # bins\n",
    "    if not bins:\n",
    "        bins = bins_sqrt(data)\n",
    "    n, bins, _ = plt.hist(data, bins=bins, density=density)\n",
    "\n",
    "    # visuals\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Density\" if density else \"Frequency\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.gca().spines[\"top\"].set_visible(False)\n",
    "    plt.gca().spines[\"right\"].set_visible(False)\n",
    "    plt.gca().spines[\"bottom\"].set_visible(False)\n",
    "    return n, bins\n",
    "\n",
    "def scatter1d(data, xticks=None, alpha=.5, s=500, marker=\"|\", xlim=None, title=\"\", **pltargs):\n",
    "    \"\"\"Create only one axis on which to plot the data.\"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(10,1))\n",
    "    ax = fig.gca()\n",
    "    size = np.array(data, copy=False).flatten().shape\n",
    "    plt.scatter(data, np.zeros(*size), alpha=alpha, marker=marker, s=s, **pltargs)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.set_yticks([])\n",
    "    if xticks:\n",
    "        ax.set_xticks(xticks)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def duh(n, precision=2):\n",
    "    \"\"\" Takes a number of bytes and returns a human-readable string with the\n",
    "    size in bytes, kilobytes, megabytes, or gigabytes.\n",
    "\n",
    "    Parameters\n",
    "        n (int): The number of bytes\n",
    "        precision (int): The number of decimals to use\n",
    "    \"\"\"\n",
    "    for unit in ['B','KB','MB','GB','TB','PB','EB','ZB']:\n",
    "        if n < 1024.0:\n",
    "            break\n",
    "        n /= 1024.0\n",
    "    decimals = precision - int(n > 99) - int(n > 9) - int(n > 0)\n",
    "    if decimals < 0 or unit == 'B':\n",
    "        decimals = 0\n",
    "    return f\"{n:.{decimals}f} {unit}\"\n",
    "\n",
    "def ising_model_graph(graph, J=(-1,1), h=(-1,1), g=(-1,1)):\n",
    "    \"\"\" Takes a graph and generates a Hamiltonian string for it that is compatible with `parse_hamiltonian`. \"\"\"\n",
    "    if not isinstance(graph, nk.graph.Graph):\n",
    "        raise ValueError(f\"graph must be a nk.graph.Graph, but is {type(graph)}\")\n",
    "    \n",
    "    # get the number of qubits\n",
    "    n_qubits = graph.n_nodes\n",
    "    # get the edges\n",
    "    edges = graph.edges()\n",
    "    # get the coupling matrix\n",
    "    J = np.array(J)\n",
    "    if J.shape == ():\n",
    "        # triangular matrix with all couplings set to J\n",
    "        J = np.triu(np.ones((n_qubits, n_qubits)), k=1) * J\n",
    "    elif J.shape == (2,):\n",
    "        # triangular matrix with all couplings set to a random value in this range\n",
    "        J = np.triu(np.random.uniform(J[0], J[1], (n_qubits, n_qubits)), k=1)\n",
    "    elif J.shape == (n_qubits, n_qubits):\n",
    "        # use the given matrix\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"J must be a scalar, 2-element vector, or matrix of shape {(n_qubits, n_qubits)}, but is {J.shape}\")\n",
    "    \n",
    "    # get the longitudinal fields\n",
    "    if h is not None:\n",
    "        h = np.array(h)\n",
    "        if h.shape == ():\n",
    "            h = np.ones(n_qubits) * h\n",
    "        elif h.shape == (2,):\n",
    "            h = np.random.uniform(h[0], h[1], n_qubits)\n",
    "        elif h.shape == (n_qubits,):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"h must be a scalar, 2-element vector, or vector of shape {(n_qubits,)}, but is {h.shape}\")\n",
    "        \n",
    "    # get the transverse fields\n",
    "    if g is not None:\n",
    "        g = np.array(g)\n",
    "        if g.shape == ():\n",
    "            g = np.ones(n_qubits) * g\n",
    "        elif g.shape == (2,):\n",
    "            g = np.random.uniform(g[0], g[1], n_qubits)\n",
    "        elif g.shape == (n_qubits,):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"g must be a scalar, 2-element vector, or vector of shape {(n_qubits,)}, but is {g.shape}\")\n",
    "        \n",
    "    # generate the Hamiltonian\n",
    "    H_str = ''\n",
    "    # pairwise interactions\n",
    "    for i, j in edges:\n",
    "        if J[i,j] != 0:\n",
    "            H_str += str(J[i,j]) + '*' + 'I'*i + 'Z' + 'I'*(j-i-1) + 'Z' + 'I'*(n_qubits-j-1) + ' + '\n",
    "    # local longitudinal fields\n",
    "    if np.any(h):\n",
    "        H_str += ' + '.join([str(h[i]) + '*' + 'I'*i + 'Z' + 'I'*(n_qubits-i-1) for i in range(n_qubits) if h[i] != 0]) + ' + '\n",
    "    # local transverse fields\n",
    "    if np.any(g):\n",
    "        H_str += ' + '.join([str(g[i]) + '*' + 'I'*i + 'X' + 'I'*(n_qubits-i-1) for i in range(n_qubits) if g[i] != 0]) + ' + '\n",
    "\n",
    "    return H_str\n",
    "\n",
    "def parse_hamiltonian(hamiltonian, sparse=False, scaling=1, buffer=None, max_buffer_n=0, dtype=float): # I'd usually default to complex, but because we're only dealing with Ising models here, float is more handy\n",
    "    \"\"\"Parse a string representation of a Hamiltonian into a matrix representation. The result is guaranteed to be Hermitian.\n",
    "\n",
    "    Parameters:\n",
    "        hamiltonian (str): The Hamiltonian to parse.\n",
    "        sparse (bool): Whether to use sparse matrices (csr_matrix) or dense matrices (numpy.array).\n",
    "        scaling (float): A constant factor to scale the Hamiltonian by.\n",
    "        buffer (dict): A dictionary to store calculated chunks in. If `None`, it defaults to the global `matmap_np` (or `matmap_sp` if `sparse == True`). Give `buffer={}` and leave `max_buffer_n == 0` (default) to disable the buffer.\n",
    "        max_buffer_n (int): The maximum length (number of qubits) for new chunks to store in the buffer (default: 0). If `0`, no new chunks will be stored in the buffer.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray | scipy.sparse.csr_matrix: The matrix representation of the Hamiltonian.\n",
    "\n",
    "    Example:\n",
    "    >>> parse_hamiltonian('0.5*(XX + YY + ZZ + II)') # SWAP\n",
    "    array([[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  1.+0.j  0.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  0.+0.j  0.+0.j  1.+0.j]])\n",
    "    >>> parse_hamiltonian('-(XX + YY + .5*ZZ) + 1.5')\n",
    "    array([[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  2.+0.j -2.+0.j  0.+0.j]\n",
    "           [ 0.+0.j -2.+0.j  2.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  0.+0.j  0.+0.j  1.+0.j]])\n",
    "    >>> parse_hamiltonian('0.5*(II + ZI - ZX + IX)') # CNOT\n",
    "\n",
    "    \"\"\"\n",
    "    kron = sp.kron if sparse else np.kron\n",
    "    matmap = matmap_sp if sparse else matmap_np\n",
    "\n",
    "    # only use buffer if pre-computed chunks are available or if new chunks are allowed to be stored\n",
    "    use_buffer = buffer is None or len(buffer) > 0 or max_buffer_n > 0\n",
    "    if use_buffer and buffer is None:\n",
    "        if dtype != float:\n",
    "            # convert matmap to the given dtype\n",
    "            matmap = {k: v.astype(dtype) for k, v in matmap.items() if \"Y\" not in k}\n",
    "        buffer = matmap\n",
    "\n",
    "    def calculate_chunk_matrix(chunk, sparse=False, scaling=1):\n",
    "        if use_buffer:\n",
    "            if chunk in buffer:\n",
    "                return buffer[chunk] if scaling == 1 else scaling * buffer[chunk]\n",
    "            if len(chunk) == 1:\n",
    "                return matmap[chunk[0]] if scaling == 1 else scaling * matmap[chunk[0]]\n",
    "            # Check if a part of the chunk has already been calculated\n",
    "            for i in range(len(chunk)-1, 1, -1):\n",
    "                for j in range(len(chunk)-i+1):\n",
    "                    subchunk = chunk[j:j+i]\n",
    "                    if subchunk in buffer:\n",
    "                        # If so, calculate the rest of the chunk recursively\n",
    "                        parts = [chunk[:j], subchunk, chunk[j+i:]]\n",
    "                        # remove empty chunks\n",
    "                        parts = [c for c in parts if c != \"\"]\n",
    "                        # See where to apply the scaling\n",
    "                        shortest = min(parts, key=len)\n",
    "                        # Calculate each in tomultiply recursively\n",
    "                        for i, c in enumerate(parts):\n",
    "                            if c == subchunk:\n",
    "                                parts[i] = buffer[c]\n",
    "                            parts[i] = calculate_chunk_matrix(c, sparse=sparse, scaling=scaling if c == shortest else 1)\n",
    "                        return reduce(kron, parts)\n",
    "\n",
    "        # Calculate the chunk matrix gate by gate\n",
    "        if use_buffer and len(chunk) <= max_buffer_n:\n",
    "            gates = [matmap[gate] for gate in chunk]\n",
    "            chunk_matrix = reduce(kron, gates)\n",
    "            buffer[chunk] = chunk_matrix\n",
    "            if scaling != 1:\n",
    "                chunk_matrix = scaling * chunk_matrix\n",
    "        else:\n",
    "            gates = [scaling * matmap[chunk[0]]] + [matmap[gate] for gate in chunk[1:]]\n",
    "            chunk_matrix = reduce(kron, gates)\n",
    "\n",
    "        return chunk_matrix\n",
    "\n",
    "    # Remove whitespace\n",
    "    hamiltonian = hamiltonian.replace(\" \", \"\")\n",
    "    # replace - with +-, except before e\n",
    "    hamiltonian = hamiltonian \\\n",
    "                    .replace(\"-\", \"+-\") \\\n",
    "                    .replace(\"e+-\", \"e-\") \\\n",
    "                    .replace(\"(+-\", \"(-\")\n",
    "\n",
    "    # print(\"parse_hamiltonian: Pre-processed Hamiltonian:\", hamiltonian)\n",
    "\n",
    "    chunks = hamiltonian.split(\"+\")\n",
    "    # Remove empty chunks\n",
    "    chunks = [c for c in chunks if c != \"\"]\n",
    "    # Use the first chunk to determine the number of qubits\n",
    "    first_chunk = chunks[0]\n",
    "    if first_chunk[0] in [\"-\", \"+\"]:\n",
    "        first_chunk = first_chunk[1:]\n",
    "    try:\n",
    "        n = len(first_chunk.split(\"*\")[1])\n",
    "    except IndexError:\n",
    "        n = len(first_chunk)\n",
    "\n",
    "    if sparse:\n",
    "        H = sp.csr_array((2**n, 2**n), dtype=dtype)\n",
    "    else:\n",
    "        if n > 10:\n",
    "            raise ValueError(f\"Using a dense matrix for a {n}-qubit Hamiltonian is not recommended. Use sparse=True.\")\n",
    "        H = np.zeros((2**n, 2**n), dtype=dtype)\n",
    "\n",
    "    for chunk in chunks:\n",
    "\n",
    "        # print(\"Processing chunk:\", chunk)\n",
    "        chunk_matrix = None\n",
    "        if chunk == \"\":\n",
    "            continue\n",
    "        # Parse the weight of the chunk\n",
    "        try:\n",
    "            if len(chunk) == n:\n",
    "                weight = 1\n",
    "            elif \"*\" in chunk:\n",
    "                weight = float(chunk.split(\"*\")[0])\n",
    "                chunk = chunk.split(\"*\")[1]\n",
    "            elif \"part\" in chunk:\n",
    "                weight = 1\n",
    "            elif len(chunk) == n+1 and chunk[0] in [\"-\", \"+\"]:\n",
    "                weight = float(chunk[0] + \"1\")\n",
    "                chunk = chunk[1:]\n",
    "            else:\n",
    "                weight = float(chunk)\n",
    "                chunk_matrix = weight * np.eye(2**n, dtype=dtype)\n",
    "        except ValueError:\n",
    "                raise ValueError(f\"Invalid chunk for size {n}: {chunk}\")\n",
    "\n",
    "        # If the chunk is a part, add it to the Hamiltonian\n",
    "        if chunk_matrix is None:\n",
    "            if len(chunk) != n:\n",
    "                raise ValueError(f\"Gate count must be {n} but was {len(chunk)} for chunk \\\"{chunk}\\\"\")\n",
    "            chunk_matrix = calculate_chunk_matrix(chunk, sparse=sparse, scaling = scaling * weight)\n",
    "\n",
    "        # Add the chunk to the Hamiltonian\n",
    "        # print(\"Adding chunk\", weight, chunk, \"to hamiltonian\", scaling, hamiltonian)\n",
    "        # print(type(H), H.dtype, type(chunk_matrix), chunk_matrix.dtype)\n",
    "        if len(chunks) == 1:\n",
    "            H = chunk_matrix\n",
    "        else:\n",
    "            H += chunk_matrix\n",
    "\n",
    "    if sparse:\n",
    "        assert np.allclose(H.data, H.conj().T.data), f\"The given Hamiltonian {hamiltonian} is not Hermitian: {H.data}\"\n",
    "    else:\n",
    "        assert np.allclose(H, H.conj().T), f\"The given Hamiltonian {hamiltonian} is not Hermitian: {H}\"\n",
    "\n",
    "    return H\n",
    "\n",
    "def calculate_rdms(state, n):\n",
    "    \"\"\" Calculate reduced density matrix for each node and each edge in the fully connected graph. \"\"\"\n",
    "\n",
    "    # calculate reduced density matrix for each node\n",
    "    node_rdms = []\n",
    "    for i in range(n):\n",
    "        rdm_i = partial_trace(state, i)\n",
    "        # reshape to a matrix\n",
    "        node_rdms.append(np.array(rdm_i))\n",
    "    # calculate reduced density matrix for each edge\n",
    "    edge_rdms = []\n",
    "    for i,j in itertools.combinations(range(n), 2):\n",
    "        rdm_ij = partial_trace(state, [i,j])\n",
    "        edge_rdms.append(np.array(rdm_ij))\n",
    "    return np.array(node_rdms), np.array(edge_rdms)\n",
    "\n",
    "def get_n_qubits(hamiltonian):\n",
    "    if isinstance(hamiltonian, str):\n",
    "        H = parse_hamiltonian(hamiltonian, sparse=True)\n",
    "    elif isinstance(hamiltonian, nk.operator.DiscreteOperator):\n",
    "        H = hamiltonian.to_sparse().astype(complex)\n",
    "    else:\n",
    "        H = hamiltonian\n",
    "    return int(np.log2(H.shape[0]))\n",
    "\n",
    "# from https://qiskit.org/documentation/stable/0.35/stubs/qiskit.quantum_info.partial_trace.html\n",
    "def partial_trace(rho, retain_qubits):\n",
    "    \"\"\"Trace out all qubits not specified in `retain_qubits`.\"\"\"\n",
    "    rho = np.array(rho)\n",
    "    n = int(np.log2(rho.shape[0])) # number of qubits\n",
    "\n",
    "    # pre-process retain_qubits\n",
    "    if isinstance(retain_qubits, int):\n",
    "        retain_qubits = [retain_qubits]\n",
    "    dim_r = 2**len(retain_qubits)\n",
    "\n",
    "    # get qubits to trace out\n",
    "    trace_out = np.array(sorted(set(range(n)) - set(retain_qubits)))\n",
    "    # ignore all qubits >= n\n",
    "    trace_out = trace_out[trace_out < n]\n",
    "\n",
    "    # if rho is a state vector\n",
    "    if len(rho.shape) == 1:\n",
    "        st  = rho.reshape([2]*n)\n",
    "        rho = np.tensordot(st, st.conj(), axes=(trace_out,trace_out))\n",
    "    # if trace out all qubits, just return the normal trace\n",
    "    elif len(trace_out) == n:\n",
    "        return np.trace(rho).reshape(1,1) # dim_r is not necessarily 1 here (if some in `retain_qubits` are >= n)\n",
    "    else:\n",
    "        assert rho.shape[0] == rho.shape[1], f\"Can't trace a non-square matrix {rho.shape}\"\n",
    "\n",
    "        rho = rho.reshape([2]*(2*n))\n",
    "        for qubit in trace_out:\n",
    "            rho = np.trace(rho, axis1=qubit, axis2=qubit+n)\n",
    "            n -= 1         # one qubit less\n",
    "            trace_out -= 1 # rename the axes (only \"higher\" ones are left)\n",
    "\n",
    "    return rho.reshape(dim_r, dim_r)\n",
    "\n",
    "def edges_from_graph(graph, undirected=False):\n",
    "    edges = graph.edges()\n",
    "    edges = np.array(edges).T\n",
    "    if undirected:\n",
    "        edges = np.concatenate([edges, edges[:,::-1]], axis=0)\n",
    "    edges = np.unique(edges, axis=0)  # sorts the edges\n",
    "    return edges[0], edges[1]\n",
    "\n",
    "# from https://docs.pennylane.ai/en/stable/code/api/pennylane.pauli_decompose.html\n",
    "def pauli_decompose(H):\n",
    "    r\"\"\"Decomposes a Hermitian matrix into a linear combination of Pauli operators.\n",
    "\n",
    "    Args:\n",
    "        H (array[complex]): a Hermitian matrix of dimension :math:`2^n\\times 2^n`.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[float], list[str]]: the coefficients and the Pauli operator strings\n",
    "\n",
    "    **Example:**\n",
    "\n",
    "    We can use this function to compute the Pauli operator decomposition of an arbitrary Hermitian\n",
    "    matrix:\n",
    "\n",
    "    >>> H = np.array(\n",
    "    ... [[-2, -2+1j, -2, -2], [-2-1j,  0,  0, -1], [-2,  0, -2, -1], [-2, -1, -1,  0]])\n",
    "    >>> pauli_decompose(H)\n",
    "    ([-1.0, -1.5, -0.5, -1.0, -1.5, -1.0, -0.5,  1.0, -0.5, -0.5],\n",
    "     ['II', 'IX', 'IY', 'IZ', 'XI', 'XX', 'XZ', 'YY', 'ZX', 'ZY'])\n",
    "    \"\"\"\n",
    "    n = int(np.log2(len(H)))\n",
    "    N = 2**n\n",
    "\n",
    "    if H.shape != (N, N):\n",
    "        raise ValueError(\"The matrix should have shape (2**n, 2**n), for any qubit number n>=1\")\n",
    "\n",
    "    if not np.allclose(H, H.conj().T):\n",
    "        raise ValueError(\"The matrix is not Hermitian\")\n",
    "\n",
    "    obs_lst = []\n",
    "    coeffs = []\n",
    "\n",
    "    for term in itertools.product(['I', 'X', 'Y', 'Z'], repeat=n):\n",
    "        term = \"\".join(term)\n",
    "        basis_matrix = reduce(np.kron, [matmap_np[i] for i in term])\n",
    "        coeff = np.trace(basis_matrix @ H) / N  # project H onto the basis matrix\n",
    "        coeff = np.real_if_close(coeff).item()\n",
    "\n",
    "        if not np.allclose(coeff, 0):\n",
    "            coeffs.append(coeff)\n",
    "            obs_lst.append(term)\n",
    "\n",
    "    return coeffs, obs_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random ising hamiltonians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nk.operator.Ising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random ising hamiltonians\n",
    "def random_ising_nk(N: int, graph: nk.graph.Graph):\n",
    "    J = np.random.uniform(-1, 1, size=N) # Coupling constant\n",
    "    g = np.random.uniform(-1, 1, size=N) # Transverse field\n",
    "    n = graph.n_nodes\n",
    "    edges = edges_from_graph(graph)\n",
    "    hilbert = nk.hilbert.Spin(s=0.5, N=n)\n",
    "    h = np.zeros(n)\n",
    "    n_ones = np.ones(n)\n",
    "\n",
    "    hamiltonians = []\n",
    "    for i in range(N):\n",
    "        ising = nk.operator.Ising(\n",
    "            hilbert=hilbert,\n",
    "            graph=graph,\n",
    "            J=J[i], h=g[i]\n",
    "        )\n",
    "\n",
    "        # Convert hyperparameters to the right format\n",
    "        J_i = np.zeros((n, n))\n",
    "        J_i[edges] = J[i]\n",
    "    \n",
    "        hamiltonians.append((ising, {\"J\": J_i, \"h\": h, \"g\": g[i]*n_ones}))\n",
    "    return hamiltonians\n",
    "\n",
    "# Generate a random ising hamiltonian\n",
    "# imshow(random_ising_nk(1, graph=nk.graph.Grid((2,3), pbc=True))[0][0].to_dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ising_model_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the ising_model_graph function to generate more general ising models\n",
    "def random_ising_own(N: int, graph: nk.graph.Graph):\n",
    "    \"\"\" Generates N random ising models on the given graph. \"\"\"\n",
    "    n = graph.n_nodes\n",
    "    J = np.random.uniform(-1, 1, size=(N, n, n))\n",
    "    # make sure each J is symmetric\n",
    "    for i in range(N):\n",
    "        J[i] = (J[i] + J[i].T)/2\n",
    "        # make sure the diagonal is zero\n",
    "        J[i] -= np.diag(np.diag(J[i]))\n",
    "    h = np.random.uniform(-1, 1, size=(N, n))\n",
    "    g = np.random.uniform(-1, 1, size=(N, n))\n",
    "\n",
    "    # get the edges for the coupling matrix\n",
    "    edges = edges_from_graph(graph)\n",
    "\n",
    "    hamiltonians = []\n",
    "    for i in range(N):\n",
    "        H_ising_str = ising_model_graph(graph, J[i], h[i], g[i])\n",
    "\n",
    "        # create the coupling matrix\n",
    "        J_i = np.zeros((n, n))\n",
    "        J_i[edges] = J[i][edges]\n",
    "    \n",
    "        hamiltonians.append((H_ising_str, {\"J\": J_i, \"h\": h[i], \"g\": g[i]}))\n",
    "\n",
    "    return hamiltonians\n",
    "\n",
    "# This creates a hamiltonian like netket does, but using the ising_model function\n",
    "def random_ising_own_like_nk(N: int, graph: nk.graph.Graph):\n",
    "    \"\"\" The main differences with generate_random_ising is (1) the coupling and field strengths are everywhere the same and (2) there is no longitudinal local field.\"\"\"\n",
    "    J = np.random.uniform(-1, 1, size=N)\n",
    "    g = np.random.uniform(-1, 1, size=N)\n",
    "    n = graph.n_nodes\n",
    "    h = np.zeros(n)\n",
    "    ones = np.ones(n)\n",
    "\n",
    "    # get the edges for the coupling matrix\n",
    "    edges = edges_from_graph(graph)\n",
    "\n",
    "    hamiltonians = []\n",
    "    for i in range(N):\n",
    "        H_ising_str = ising_model_graph(graph, J[i], 0, g[i])\n",
    "\n",
    "        # create the coupling and field matrices\n",
    "        J_i = np.zeros((n,n))\n",
    "        J_i[edges] = J[i]\n",
    "\n",
    "        hamiltonians.append((H_ising_str, {\"J\": J_i, \"h\": h, \"g\": g[i]*ones}))\n",
    "\n",
    "    return hamiltonians\n",
    "\n",
    "# show a random hamiltonian\n",
    "# imshow(parse_hamiltonian(random_ising_own(N=1, graph=nk.graph.Grid((2,3), pbc=True))[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check equivalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nk.graph.Grid((2,3), pbc=True)\n",
    "J = np.random.uniform(-1, 1)\n",
    "h = np.random.uniform(-1, 1)\n",
    "\n",
    "# check if the Hilbert space dimension is too large (happens too easily)\n",
    "H_dim = 2**(graph.n_nodes)\n",
    "mem_required = H_dim**2 *4\n",
    "if mem_required > psutil.virtual_memory().available:\n",
    "    raise ValueError(f\"Warning: Large Hilbert space dimension {H_dim}! This would blow up your memory ({duh(mem_required)} bytes required)!\")\n",
    "elif H_dim > 2**11:\n",
    "    print(f\"Warning: Large Hilbert space dimension {H_dim}! This may take a while ({duh(mem_required)} bytes required)!\")\n",
    "\n",
    "# netket version of the Ising model\n",
    "ising_nk = nk.operator.Ising(\n",
    "    hilbert=nk.hilbert.Spin(s=0.5, N=graph.n_nodes),\n",
    "    graph=graph,\n",
    "    J=J, h=h\n",
    ").to_dense()\n",
    "# corresponds to the following special case\n",
    "H_str = ising_model_graph(graph, J, 0, -h)\n",
    "ising_own = parse_hamiltonian(H_str)\n",
    "\n",
    "np.allclose(ising_nk, ising_own)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate ground state energies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact diagonalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonalize(hamiltonian, *args):\n",
    "    if isinstance(hamiltonian, str):\n",
    "        H = parse_hamiltonian(hamiltonian, sparse=True)\n",
    "    elif isinstance(hamiltonian, nk.operator.DiscreteOperator):\n",
    "        H = hamiltonian.to_sparse()\n",
    "    else:\n",
    "        H = hamiltonian\n",
    "\n",
    "    evals, evecs = sp.linalg.eigsh(H, k=1, which='SA')\n",
    "    ground_state_energy = evals[0]\n",
    "    ground_state = evecs[:,0]\n",
    "    return ground_state_energy, ground_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The actual generation is happening here!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(hamiltonians, data_path=None):\n",
    "    \"\"\"Generate the data set using exact diagonalization. This function does three things:\n",
    "    1. Diagonalize the given Hamiltonians using exact diagonalization.\n",
    "    2. Calculate the reduced density matrix of the ground state for each node and each edge in the fully connect graph.\n",
    "    3. Save the data set to a file specified by data_path (optional).\n",
    "\n",
    "    Previously generated data set can be extended by passing the path to the existing data set to data_path.\n",
    "    If `data_path` is passed and data is found, but no Hamiltonians are given, the existing data set is returned.\n",
    "\n",
    "    Parameters\n",
    "        hamiltonians: list of 2-tuples, with Hamiltonians to be diagonalized (need to have all the same number dimension) and their hyperparameters\n",
    "        data_path: path to save the data set\n",
    "\n",
    "    Returns\n",
    "        data: structured array containing the data set\n",
    "    \"\"\"\n",
    "    if data_path is not None:\n",
    "        # add .npy extension if not present\n",
    "        if not data_path.endswith('.npy'):\n",
    "            data_path += '.npy'\n",
    "        if os.path.exists(data_path):\n",
    "            # load existing data\n",
    "            data_old = np.load(data_path, allow_pickle=True)\n",
    "        else:\n",
    "            os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
    "            data_old = None\n",
    "    if len(hamiltonians) == 0:\n",
    "        if data_path is not None and data_old is not None:\n",
    "            return data_old\n",
    "        else:\n",
    "            raise ValueError('No Hamiltonians given and no already generated data set found.')\n",
    "\n",
    "    # get number of qubits\n",
    "    n = get_n_qubits(hamiltonians[0][0])\n",
    "    # create a structured array to store the data\n",
    "    data = np.empty(len(hamiltonians), dtype=[\n",
    "        ('hamiltonian', 'O'),                 # hamiltonian hyperparameters\n",
    "        ('energy', 'f8'),                     # ground state energy\n",
    "        # ('state', 'f8', (2**n,)),           # ground state (warning: blows up the data set size)\n",
    "        ('node_rdm', 'f8', (n,2,2)),          # reduced density matrix for each node\n",
    "        ('edge_rdm', 'f8', (n*(n-1)//2,4,4))  # reduced density matrix for each edge\n",
    "    ])\n",
    "    for i in tq(range(len(hamiltonians)), desc='Generating data set'):\n",
    "        # get Hamiltonian\n",
    "        hamiltonian, hyperparameters = hamiltonians[i]\n",
    "        # get ground state energy and state\n",
    "        energy, state = diagonalize(hamiltonian)\n",
    "        # calculate reduced density matrix for each node and each edge\n",
    "        if data_path is not None:\n",
    "            node_rdms, edge_rdms = calculate_rdms(state, n)\n",
    "            # add data to structured array\n",
    "            # data[i] = (hyperparameters, energy, state, node_rdms, edge_rdms)\n",
    "            data[i] = (hyperparameters, energy, node_rdms, edge_rdms)\n",
    "\n",
    "    if data_path is not None:\n",
    "        if data_old is not None:\n",
    "            # merge old and new data\n",
    "            data = np.concatenate([data_old, data])\n",
    "            # remove duplicates\n",
    "            # data = np.unique(data, axis=0)\n",
    "        np.save(data_path, data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "n = (12,) # System size\n",
    "          # Timing for exact diagonalization (on M1 chip)\n",
    "          # (8,) => 34000 / min, (12,) => 1500 / min, (16,) => 100 / min, (20,) => 4 / min\n",
    "          # (2,4) => 36000 /min, (3,3) => 8000 / min, (3,4) => 1500 / min, (4,4) => 100 / min, (4,5) => 6 / min\n",
    "          # (2,2,2) => 40000 / min, (2,2,3) => 1500 / min, (2,3,3) => 30 / min\n",
    "circular = False # Whether to use periodic boundaries\n",
    "N = 10000          # Number of samples to generate\n",
    "\n",
    "# Generate\n",
    "graph = nk.graph.Grid(n, pbc=circular)\n",
    "data = generate_dataset(random_ising_nk(N, graph), data_path=f'data/nk_{N}_{n}_{circular}')\n",
    "\n",
    "# Plot\n",
    "_ = hist(data['energy'], xlabel=\"Ground state energy\", title=f\"Ground state energy of {len(data)} random ising models with n={n} (circ={circular})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "n = (12,) # System size\n",
    "          # Timing for exact diagonlization (on M1 chip):\n",
    "          # (8,) => 9000/min, (10,) => 3000/min, (12,) => 1000/min, (14,) => 330/min, (16,) => 90/min, (20,) => 4/min\n",
    "          # (2,3) => 20000/min, (2,4) => 8500/min, (3,3) => 5000/min, (3,4) => 1000/min, (4,4) => 70/min, (4,5) => 4/min\n",
    "          # (2,2,2) => 8000/min, (2,2,3) => 800/min, (2,3,3) => 20/min\n",
    "circular = False  # Circular boundary\n",
    "N = 100           # Number of samples to generate\n",
    "\n",
    "# Generate\n",
    "graph = nk.graph.Grid(n, pbc=circular)\n",
    "data = generate_dataset(random_ising_own(N, graph), data_path=f'data/own_{n}_{circular}')\n",
    "\n",
    "# Plot\n",
    "_ = hist(data['energy'], xlabel=\"Ground state energy\", title=f\"Ground state energy of {len(data)} random ising models with n={n} (circ={circular})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit() # here to prevent the notebook from running further and destroying the timings and plots when running all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full procedure for generating the dataset for one hamiltonian\n",
    "%timeit diagonalize(random_ising_nk(1, nk.graph.Grid((12,), pbc=False))[0][0].to_sparse())\n",
    "%timeit diagonalize(parse_hamiltonian(random_ising_own(1, nk.graph.Grid((12,), pbc=False))[0][0], sparse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nk.graph.Grid((8,), pbc=False)\n",
    "\n",
    "print(f\"Abstract Hamiltonian object ({graph.n_nodes} qubits)\")\n",
    "%timeit random_ising_nk(1, graph)\n",
    "%timeit random_ising_own(1, graph)\n",
    "\n",
    "print(f\"Sparse Hamiltonian matrix ({graph.n_nodes} qubits)\")\n",
    "%timeit random_ising_nk(1, graph)[0][0].to_sparse()\n",
    "%timeit parse_hamiltonian(random_ising_own(1, graph)[0][0], sparse=True)\n",
    "\n",
    "H1 = random_ising_nk(1, graph)[0][0].to_sparse()\n",
    "H2 = parse_hamiltonian(random_ising_own(1, graph)[0][0], sparse=True)\n",
    "\n",
    "print(f\"Diagonalizing the Hamiltonian ({graph.n_nodes} qubits)\")\n",
    "%timeit diagonalize(H1)\n",
    "%timeit diagonalize(H2)\n",
    "\n",
    "graph = nk.graph.Grid((12,), pbc=False)\n",
    "\n",
    "print(f\"Abstract Hamiltonian object ({graph.n_nodes} qubits)\")\n",
    "%timeit random_ising_nk(1, graph)\n",
    "%timeit random_ising_own(1, graph)\n",
    "\n",
    "print(f\"Sparse Hamiltonian matrix ({graph.n_nodes} qubits)\")\n",
    "%timeit random_ising_nk(1, graph)[0][0].to_sparse()\n",
    "%timeit parse_hamiltonian(random_ising_own(1, graph)[0][0], sparse=True)\n",
    "\n",
    "H1 = random_ising_nk(1, graph)[0][0].to_sparse()\n",
    "H2 = parse_hamiltonian(random_ising_own(1, graph)[0][0], sparse=True)\n",
    "\n",
    "print(f\"Diagonalizing the Hamiltonian ({graph.n_nodes} qubits)\")\n",
    "%timeit diagonalize(H1)\n",
    "%timeit diagonalize(H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nk.graph.Grid((8,), pbc=False)\n",
    "# constructing the abstract representations of the hamiltonians\n",
    "print(f\"Abstract representations ({graph.n_nodes} qubits)\")\n",
    "%timeit len(list(random_ising_nk(N=100, graph=graph)))\n",
    "%timeit len(list(random_ising_own(N=100, graph=graph)))\n",
    "# creating the actual matrices\n",
    "print(f\"Matrices ({graph.n_nodes} qubits)\")\n",
    "%timeit len(list(h.to_sparse() for h, _ in random_ising_nk(N=100, graph=graph)))\n",
    "%timeit len(list(parse_hamiltonian(h, sparse=True) for h, _ in random_ising_own(N=100, graph=graph)))\n",
    "\n",
    "graph = nk.graph.Grid((12,), pbc=False)\n",
    "# constructing the abstract representations of the hamiltonians\n",
    "print(f\"Abstract representations ({graph.n_nodes} qubits)\")\n",
    "%timeit len(list(random_ising_nk(N=100, graph=graph)))\n",
    "%timeit len(list(random_ising_own(N=100, graph=graph)))\n",
    "# creating the actual matrices\n",
    "print(f\"Matrices ({graph.n_nodes} qubits)\")\n",
    "%timeit len(list(h.to_sparse() for h, _ in random_ising_nk(N=100, graph=graph)))\n",
    "%timeit len(list(parse_hamiltonian(h, sparse=True) for h, _ in random_ising_own(N=100, graph=graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean field ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import json\n",
    "\n",
    "def save_data(specifier, cb=None):\n",
    "    path = f\"data/data_{specifier}.json\"\n",
    "    def _save(arg):\n",
    "        if cb is not None:\n",
    "            key, value = cb(arg)\n",
    "        else:\n",
    "            key, value = arg\n",
    "        # append to json dict\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "        data[key] = value\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    return _save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Input: x.shape = (n_samples, L)\n",
    "        Output: log_psi.shape = (n_samples,)\n",
    "        \"\"\"\n",
    "        lam = self.param(\n",
    "            \"lambda\", nn.initializers.normal(), (1,), float\n",
    "        )\n",
    "        p = nn.log_sigmoid(lam*x)\n",
    "        return 0.5 * jnp.sum(p, axis=-1)\n",
    "    \n",
    "def mean_field(hamiltonian, n_samples=1000, n_iter=300, logfile='test', show_progress=False):\n",
    "    vstate = nk.vqs.MCState(\n",
    "        sampler=nk.sampler.MetropolisLocal(hamiltonian.hilbert),\n",
    "        model=MF(),\n",
    "        n_samples=n_samples,\n",
    "    )\n",
    "    optimizer = nk.optimizer.Sgd(learning_rate=0.05)\n",
    "    gs = nk.driver.VMC(hamiltonian, optimizer, variational_state=vstate)\n",
    "    gs.run(n_iter=n_iter, out=logfile, show_progress=show_progress)\n",
    "    energy = vstate.expect(hamiltonian)\n",
    "    # Check if converged\n",
    "    if np.abs(energy.R_hat - 1) > 0.1:\n",
    "        print('Not converged!', energy)\n",
    "        return str(hamiltonian), np.nan\n",
    "\n",
    "    return str(hamiltonian), float(energy.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "def plot_json(filename, y, x, exact_energy):\n",
    "    import json\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    data = data[\"Energy\"]\n",
    "    plt.plot(data[x], data[y])\n",
    "    plt.axhline(exact_energy, color='r', linestyle='--')\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.show()\n",
    "\n",
    "hamiltonian = generate_nk_ising(1, 12)[0][0]\n",
    "print(hamiltonian)\n",
    "mean_field(hamiltonian, n_samples=1000, n_iter=1000, logfile='test', show_progress=True)\n",
    "# Also calculate exact energy\n",
    "exact_energy = sp.linalg.eigsh(hamiltonian.to_sparse().astype(complex), k=1, which='SA')[0][0]\n",
    "# Read JSON from \"test.log\" and plot \"Energy.mean\" against \"Energy.iters\"\n",
    "plot_json('test.log', 'Mean', 'iters', exact_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_iter == 300` seems like a good choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VMC (Jastrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jastrow(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Input: x.shape = (n_samples, L)\n",
    "        Output: log_psi.shape = (n_samples,)\n",
    "        \"\"\"\n",
    "        lam1 = self.param(\n",
    "            \"lambda1\", nn.initializers.normal(), (1,), float\n",
    "        )\n",
    "        lam2 = self.param(\n",
    "            \"lambda2\", nn.initializers.normal(), (1,), float\n",
    "        )\n",
    "        # compute the nearest-neighbor correlations\n",
    "        corr1=x*jnp.roll(x,-1,axis=-1)\n",
    "        corr2=x*jnp.roll(x,-2,axis=-1)\n",
    "\n",
    "        # sum the output\n",
    "        return jnp.sum(lam1*corr1+lam2*corr2,axis=-1)\n",
    "\n",
    "def jastrow(hamiltonian, n_samples=1000, n_iter=30, logfile='test', show_progress=False):\n",
    "    vstate = nk.vqs.MCState(\n",
    "        sampler=nk.sampler.MetropolisLocal(hamiltonian.hilbert),\n",
    "        model=Jastrow(),\n",
    "        n_samples=n_samples,\n",
    "    )\n",
    "    optimizer = nk.optimizer.Sgd(learning_rate=0.05)\n",
    "    gs = nk.driver.VMC(hamiltonian, optimizer, variational_state=vstate)\n",
    "    gs.run(n_iter=n_iter, out=logfile, show_progress=show_progress)\n",
    "    energy = vstate.expect(hamiltonian)\n",
    "    # Check if converged\n",
    "    if np.abs(energy.R_hat - 1) > 0.1:\n",
    "        print('Not converged!', energy)\n",
    "        return str(hamiltonian), np.nan\n",
    "\n",
    "    return str(hamiltonian), float(energy.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "def plot_json(filename, y, x, exact_energy):\n",
    "    import json\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    data = data[\"Energy\"]\n",
    "    plt.plot(data[x], data[y])\n",
    "    plt.axhline(exact_energy, color='r', linestyle='--')\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.show()\n",
    "\n",
    "hamiltonian = generate_nk_ising(1, 12)[0][0]\n",
    "print(hamiltonian)\n",
    "jastrow(hamiltonian, n_samples=1000, n_iter=300, logfile='test', show_progress=True)\n",
    "# Also calculate exact energy\n",
    "exact_energy = sp.linalg.eigsh(hamiltonian.to_sparse().astype(complex), k=1, which='SA')[0][0]\n",
    "# Read JSON from \"test.log\" and plot \"Energy.mean\" against \"Energy.iters\"\n",
    "plot_json('test.log', 'Mean', 'iters', exact_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it converges, it usually takes `10-20` iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_energies = {}\n",
    "mean_field_energies = {}\n",
    "jastrow_energies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "\n",
    "N = 1000\n",
    "L = 12\n",
    "name = \"12nk\"\n",
    "hamiltonians_test = random_ising_nk(N, graph=nk.graph.Grid((L,)))\n",
    "\n",
    "if L <= 23: # Max size with 16GB RAM\n",
    "    exact_energies[name] = mapp(diagonalize, hamiltonians_test)\n",
    "    scatter1d([e for e,s in exact_energies[name]], title='Exact ground state energies')\n",
    "\n",
    "mean_field_energies[name] = mapp(mean_field, hamiltonians_test[::10], cb=save_data(f'mf_{L}'))\n",
    "scatter1d([e for _,e in mean_field_energies[name]], title='Mean field ground state energies')\n",
    "\n",
    "jastrow_energies[name] = mapp(jastrow, hamiltonians_test, cb=save_data(f'jastrow_{L}'))\n",
    "scatter1d([e for _,e in jastrow_energies[name]], title='Jastrow ground state energies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this are some generated figures that need quite some time to generate so I didn't want to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "\n",
    "N = 20000\n",
    "L = 12\n",
    "name = \"12nk\"\n",
    "hamiltonians_test = random_ising_nk(N, graph=graph=nk.graph.Grid((L,)))\n",
    "\n",
    "if L <= 23: # Max size with 16GB RAM\n",
    "    exact_energies[name] = mapp(diagonalize, hamiltonians_test, cb=save_data(f'nk_{L}'))\n",
    "    hist([e for e,s in exact_energies[name]], title='Exact ground state energies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "\n",
    "N = 20000\n",
    "n = (2,3)\n",
    "name = \"6is\"\n",
    "hamiltonians_test = random_ising_own(N, graph=nk.graph.Grid(n))\n",
    "\n",
    "if np.prod(n) <= 23: # Max size with 16GB RAM\n",
    "    exact_energies[name] = mapp(diagonalize, hamiltonians_test) # , cb=save_data(f'{n}')\n",
    "    hist([e for e,s in exact_energies[name]], title='Exact ground state energies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "\n",
    "N = 200\n",
    "L = 30\n",
    "name = \"30nk\"\n",
    "hamiltonians_test = random_ising_nk(N, graph=nk.graph.Grid((L,)))\n",
    "\n",
    "jastrow_energies[name] = mapp(jastrow, hamiltonians_test, cb=save_data(f'jastrow_{L}'))\n",
    "hist([e for e,s in jastrow_energies[name]], title='Jastrow ground state energies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "\n",
    "N = 2000\n",
    "L = 20\n",
    "name = f\"{L}nk\"\n",
    "hamiltonians_test = random_ising_nk(N, graph=nk.graph.Grid((L,)))\n",
    "\n",
    "exact_energies[name] = mapp(diagonalize, hamiltonians_test, cb=save_data(f'nk_{L}'))\n",
    "hist([e for e,s in exact_energies[name]], title='Exact ground state energies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect generated data\n",
    "(only for the old json format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/data_jastrow_30.json', 'r') as f:\n",
    "with open('data/data_nk_12.json', 'r') as f:\n",
    "# with open('data/data_(2, 3).json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    # sort by energy\n",
    "    data_s = sorted(data.items(), key=lambda x: x[1] if not np.isnan(x[1]) else -np.inf)\n",
    "    print(\"**\",len(data), \"entries **\")\n",
    "    for H, E0 in data_s[::-1][:10]:\n",
    "        print(f\"{E0:>3.3f}\\t{H}\")\n",
    "    hist(list(data.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
