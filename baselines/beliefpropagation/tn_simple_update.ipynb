{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quimb.tensor as qtn\n",
    "import quimb as qu\n",
    "import netket as nk\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task in this notebook is to find the ground state energy of a given ising model hamiltonian using the update algorithm on tensor networks.\n",
    "For the update we use the update algorithm implemented in the `quimb` library. But first let's quickly review the basics of the update algorithm.\n",
    "\n",
    "The imaginary time evolution (ITE) operator is defined as follows:\n",
    "$$\n",
    "U(\\tau) = e^{-\\tau H}\n",
    "$$\n",
    "For $\\tau \\rightarrow \\infty$ the ITE operator maps any state to the ground state of the hamiltonian $H$. The ITE operator is not unitary (so, it has actually nothing to do with time evolution), but it works like a softmax (with $\\tau = -\\beta$) to pick out the ground state.\n",
    "$$\n",
    "\\ket{\\psi_0} \\propto \\lim_{\\tau \\rightarrow \\infty} U(\\tau) \\ket{\\psi_\\text{init}}\n",
    "$$\n",
    "To avoid huge numbers in the exponential, we iterate the ITE operator in small steps $\\delta \\tau$ until convergence.\n",
    "$$\n",
    "\\psi_{\\tau + \\delta \\tau} = \\frac{U(\\delta \\tau) \\psi_{\\tau}}{\\| U(\\delta \\tau) \\psi_{\\tau} \\|_2}\n",
    "$$\n",
    "Furthermore, the update algorithm uses the fact that a matrix exponential can be decomposed into a product of exponentials of the sum of the matrices if the matrices commute: $e^{A + B} = e^A e^B \\iff [A, B] = 0$.\n",
    "So, if we assume that the hamiltonian can be decomposed into two commuting parts $H = \\sum_i H_i$ (e.g. that operate on different parts of the system), we can decompose the ITE operator as follows:\n",
    "$$\n",
    "U(\\delta \\tau) = e^{-\\delta \\tau H} = e^{-\\delta \\tau \\sum_i H_i} = \\prod_i e^{-\\delta \\tau H_i}\n",
    "$$\n",
    "\n",
    "The update algorithm is implemented in the `quimb` library. We can use it to find the ground state energy of a given hamiltonian. Let's try it out on a simple hamiltonian:\n",
    "$$\n",
    "H = \\sigma^z_1 \\sigma^z_2 + \\sigma^z_2 \\sigma^z_3 + \\sigma^z_3 \\sigma^z_4\n",
    "$$\n",
    "The ground state energy is $E_0 = -3$ and the ground state is $\\ket{\\psi_0} = \\ket{0101}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from functools import reduce\n",
    "\n",
    "matmap_np, matmap_sp = None, None\n",
    "\n",
    "def parse_hamiltonian(hamiltonian, sparse=False, scaling=1, buffer=None, max_buffer_n=0, dtype=float): # I'd usually default to complex, but because we're only dealing with Ising models here, float is more handy\n",
    "    \"\"\"Parse a string representation of a Hamiltonian into a matrix representation. The result is guaranteed to be Hermitian.\n",
    "\n",
    "    Parameters:\n",
    "        hamiltonian (str): The Hamiltonian to parse.\n",
    "        sparse (bool): Whether to use sparse matrices (csr_matrix) or dense matrices (numpy.array).\n",
    "        scaling (float): A constant factor to scale the Hamiltonian by.\n",
    "        buffer (dict): A dictionary to store calculated chunks in. If `None`, it defaults to the global `matmap_np` (or `matmap_sp` if `sparse == True`). Give `buffer={}` and leave `max_buffer_n == 0` (default) to disable the buffer.\n",
    "        max_buffer_n (int): The maximum length (number of qubits) for new chunks to store in the buffer (default: 0). If `0`, no new chunks will be stored in the buffer.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray | scipy.sparse.csr_matrix: The matrix representation of the Hamiltonian.\n",
    "\n",
    "    Example:\n",
    "    >>> parse_hamiltonian('0.5*(XX + YY + ZZ + II)') # SWAP\n",
    "    array([[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  1.+0.j  0.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  0.+0.j  0.+0.j  1.+0.j]])\n",
    "    >>> parse_hamiltonian('-(XX + YY + .5*ZZ) + 1.5')\n",
    "    array([[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  2.+0.j -2.+0.j  0.+0.j]\n",
    "           [ 0.+0.j -2.+0.j  2.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  0.+0.j  0.+0.j  1.+0.j]])\n",
    "    >>> parse_hamiltonian('0.5*(II + ZI - ZX + IX)') # CNOT\n",
    "\n",
    "    \"\"\"\n",
    "    kron = sp.kron if sparse else np.kron\n",
    "\n",
    "    # Initialize the matrix map\n",
    "    global matmap_np, matmap_sp\n",
    "    if matmap_np is None or matmap_sp is None or matmap_np[\"I\"].dtype != dtype:\n",
    "        # numpy versions\n",
    "        matmap_np = {\n",
    "            \"H\": np.array([[1, 1], [1, -1]], dtype=dtype) / np.sqrt(2),\n",
    "            \"X\": np.array([[0, 1], [1, 0]], dtype=dtype),\n",
    "            \"Z\": np.array([[1, 0], [0, -1]], dtype=dtype),\n",
    "            \"I\": np.array([[1, 0], [0, 1]], dtype=dtype),\n",
    "        }\n",
    "        # composites\n",
    "        matmap_np.update({\n",
    "            \"ZZ\": np.kron(matmap_np['Z'], matmap_np['Z']),\n",
    "            \"IX\": np.kron(matmap_np['I'], matmap_np['X']),\n",
    "            \"XI\": np.kron(matmap_np['X'], matmap_np['I']),\n",
    "            \"YY\": np.array([[ 0,  0,  0, -1],  # to avoid complex numbers\n",
    "                            [ 0,  0,  1,  0],\n",
    "                            [ 0,  1,  0,  0],\n",
    "                            [-1,  0,  0,  0]], dtype=dtype)\n",
    "        })\n",
    "        for i in range(2, 11):\n",
    "            matmap_np[\"I\"*i] = np.eye(2**i, dtype=dtype)\n",
    "        # add 'Y' only if dtype supports imaginary numbers\n",
    "        if np.issubdtype(dtype, np.complexfloating):\n",
    "            matmap_np[\"Y\"] = np.array([[0, -1j], [1j, 0]], dtype=dtype)\n",
    "\n",
    "        # sparse versions\n",
    "        matmap_sp = {k: sp.csr_array(v) for k, v in matmap_np.items()}\n",
    "    \n",
    "    if not np.issubdtype(dtype, np.complexfloating) and \"Y\" in hamiltonian:\n",
    "        raise ValueError(f\"The Pauli matrix Y is not supported for dtype {dtype.__name__}.\")\n",
    "\n",
    "    matmap = matmap_sp if sparse else matmap_np\n",
    "\n",
    "    # only use buffer if pre-computed chunks are available or if new chunks are allowed to be stored\n",
    "    use_buffer = buffer is None or len(buffer) > 0 or max_buffer_n > 0\n",
    "    if use_buffer and buffer is None:\n",
    "        buffer = matmap\n",
    "\n",
    "    def calculate_chunk_matrix(chunk, sparse=False, scaling=1):\n",
    "        # if scaling != 1:  # only relevant for int dtype\n",
    "            # scaling = np.array(scaling, dtype=dtype)\n",
    "        if use_buffer:\n",
    "            if chunk in buffer:\n",
    "                return buffer[chunk] if scaling == 1 else scaling * buffer[chunk]\n",
    "            if len(chunk) == 1:\n",
    "                return matmap[chunk[0]] if scaling == 1 else scaling * matmap[chunk[0]]\n",
    "            # Check if a part of the chunk has already been calculated\n",
    "            for i in range(len(chunk)-1, 1, -1):\n",
    "                for j in range(len(chunk)-i+1):\n",
    "                    subchunk = chunk[j:j+i]\n",
    "                    if subchunk in buffer:\n",
    "                        # If so, calculate the rest of the chunk recursively\n",
    "                        parts = [chunk[:j], subchunk, chunk[j+i:]]\n",
    "                        # remove empty chunks\n",
    "                        parts = [c for c in parts if c != \"\"]\n",
    "                        # See where to apply the scaling\n",
    "                        shortest = min(parts, key=len)\n",
    "                        # Calculate each part recursively\n",
    "                        for i, c in enumerate(parts):\n",
    "                            if c == subchunk:\n",
    "                                if c == shortest:\n",
    "                                    parts[i] = scaling * buffer[c]\n",
    "                                    shortest = \"\"\n",
    "                                else:\n",
    "                                    parts[i] = buffer[c]\n",
    "                            else:\n",
    "                                if c == shortest:\n",
    "                                    parts[i] = calculate_chunk_matrix(c, sparse=sparse, scaling=scaling)\n",
    "                                    shortest = \"\"\n",
    "                                else:\n",
    "                                    parts[i] = calculate_chunk_matrix(c, sparse=sparse, scaling=1)\n",
    "                        return reduce(kron, parts)\n",
    "\n",
    "        # Calculate the chunk matrix gate by gate\n",
    "        if use_buffer and len(chunk) <= max_buffer_n:\n",
    "            gates = [matmap[gate] for gate in chunk]\n",
    "            chunk_matrix = reduce(kron, gates)\n",
    "            buffer[chunk] = chunk_matrix\n",
    "            if scaling != 1:\n",
    "                chunk_matrix = scaling * chunk_matrix\n",
    "        else:\n",
    "            gates = [scaling * matmap[chunk[0]]] + [matmap[gate] for gate in chunk[1:]]\n",
    "            chunk_matrix = reduce(kron, gates)\n",
    "\n",
    "        return chunk_matrix\n",
    "\n",
    "    # Remove whitespace\n",
    "    hamiltonian = hamiltonian.replace(\" \", \"\")\n",
    "    # replace - with +-, except before e\n",
    "    hamiltonian = hamiltonian \\\n",
    "                    .replace(\"-\", \"+-\") \\\n",
    "                    .replace(\"e+-\", \"e-\") \\\n",
    "                    .replace(\"(+-\", \"(-\")\n",
    "\n",
    "    # print(\"parse_hamiltonian: Pre-processed Hamiltonian:\", hamiltonian)\n",
    "\n",
    "    # Find parts in parentheses\n",
    "    part = \"\"\n",
    "    parts = []\n",
    "    depth = 0\n",
    "    current_part_weight = \"\"\n",
    "    for i, c in enumerate(hamiltonian):\n",
    "        if c == \"(\":\n",
    "            if depth == 0:\n",
    "                # for top-level parts search backwards for the weight\n",
    "                weight = \"\"\n",
    "                for j in range(i-1, -1, -1):\n",
    "                    if hamiltonian[j] in [\"(\"]:\n",
    "                        break\n",
    "                    weight += hamiltonian[j]\n",
    "                    if hamiltonian[j] in [\"+\", \"-\"]:\n",
    "                        break\n",
    "                weight = weight[::-1]\n",
    "                if weight != \"\":\n",
    "                    current_part_weight = weight\n",
    "            depth += 1\n",
    "        elif c == \")\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                part += c\n",
    "                parts.append((current_part_weight, part))\n",
    "                part = \"\"\n",
    "                current_part_weight = \"\"\n",
    "        if depth > 0: \n",
    "            part += c\n",
    "\n",
    "    # print(\"Parts found:\", parts)\n",
    "\n",
    "    # Replace parts in parentheses with a placeholder\n",
    "    for i, (weight, part) in enumerate(parts):\n",
    "        hamiltonian = hamiltonian.replace(weight+part, f\"+part{i}\", 1)\n",
    "        # remove * at the end of the weight\n",
    "        if weight != \"\" and weight[-1] == \"*\":\n",
    "            weight = weight[:-1]\n",
    "        if weight in [\"\", \"+\", \"-\"]:\n",
    "            weight += \"1\"\n",
    "        # Calculate the part recursively\n",
    "        part = part[1:-1] # remove parentheses\n",
    "        parts[i] = parse_hamiltonian(part, sparse=sparse, scaling=float(weight), buffer=buffer, max_buffer_n=max_buffer_n, dtype=dtype)\n",
    "\n",
    "    # print(\"Parts replaced:\", parts)\n",
    "\n",
    "    # Parse the rest of the Hamiltonian\n",
    "    chunks = hamiltonian.split(\"+\")\n",
    "    # Remove empty chunks\n",
    "    chunks = [c for c in chunks if c != \"\"]\n",
    "    # If parts are present, use them to determine the number of qubits\n",
    "    if parts:\n",
    "        n = int(np.log2(parts[0].shape[0]))\n",
    "    else: # Use chunks to determine the number of qubits\n",
    "        n = 0\n",
    "        for c in chunks:\n",
    "            if c[0] in [\"-\", \"+\"]:\n",
    "                c = c[1:]\n",
    "            if \"*\" in c:\n",
    "                c = c.split(\"*\")[1]\n",
    "            if c.startswith(\"part\"):\n",
    "                continue\n",
    "            try:\n",
    "                float(c)\n",
    "                continue\n",
    "            except ValueError:\n",
    "                n = len(c)\n",
    "                break\n",
    "        if n == 0:\n",
    "            print(\"Warning: Hamiltonian is a scalar!\")\n",
    "\n",
    "    if not sparse and n > 10:\n",
    "        # check if we would blow up the memory\n",
    "        mem_required = 2**(2*n) * np.array(1, dtype=dtype).nbytes\n",
    "        mem_available = psutil.virtual_memory().available\n",
    "        if mem_required > mem_available:\n",
    "            raise MemoryError(f\"This would blow up you memory ({duh(mem_required)} required)! Try using `sparse=True`.\")\n",
    "\n",
    "    if sparse:\n",
    "        H = sp.csr_array((2**n, 2**n), dtype=dtype)\n",
    "    else:\n",
    "        if n > 10:\n",
    "            print(f\"Warning: Using a dense matrix for a {n}-qubit Hamiltonian is not recommended. Use sparse=True.\")\n",
    "        H = np.zeros((2**n, 2**n), dtype=dtype)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # print(\"Processing chunk:\", chunk)\n",
    "        chunk_matrix = None\n",
    "        if chunk == \"\":\n",
    "            continue\n",
    "        # Parse the weight of the chunk\n",
    "        \n",
    "        if chunk.startswith(\"part\"):\n",
    "            weight = 1  # parts are already scaled\n",
    "            chunk_matrix = parts[int(chunk.split(\"part\")[1])]\n",
    "        elif \"*\" in chunk:\n",
    "            weight = float(chunk.split(\"*\")[0])\n",
    "            chunk = chunk.split(\"*\")[1]\n",
    "        elif len(chunk) == n+1 and chunk[0] in [\"-\", \"+\"] and n >= 1 and chunk[1] in matmap:\n",
    "            weight = float(chunk[0] + \"1\")\n",
    "            chunk = chunk[1:]\n",
    "        elif (chunk[0] in [\"-\", \"+\", \".\"] or chunk[0].isdigit()) and all([c not in matmap for c in chunk[1:]]):\n",
    "            if len(chunk) == 1 and chunk[0] in [\"-\", \".\"]:\n",
    "                chunk = 0\n",
    "            weight = complex(chunk)\n",
    "            if np.iscomplex(weight):\n",
    "                raise ValueError(\"Complex scalars would make the Hamiltonian non-Hermitian!\")\n",
    "            weight = weight.real\n",
    "            # weight = np.array(weight, dtype=dtype)  # only relevant for int dtype\n",
    "            chunk_matrix = np.eye(2**n, dtype=dtype)\n",
    "        elif len(chunk) != n:\n",
    "            raise ValueError(f\"Gate count must be {n} but was {len(chunk)} for chunk \\\"{chunk}\\\"\")\n",
    "        else:\n",
    "            weight = 1\n",
    "\n",
    "        if chunk_matrix is None:\n",
    "            chunk_matrix = calculate_chunk_matrix(chunk, sparse=sparse, scaling = scaling * weight)\n",
    "        elif scaling * weight != 1:\n",
    "            chunk_matrix = scaling * weight * chunk_matrix\n",
    "\n",
    "        # Add the chunk to the Hamiltonian\n",
    "        # print(\"Adding chunk\", weight, chunk, \"for hamiltonian\", scaling, hamiltonian)\n",
    "        # print(type(H), H.dtype, type(chunk_matrix), chunk_matrix.dtype)\n",
    "        if len(chunks) == 1:\n",
    "            H = chunk_matrix\n",
    "        else:\n",
    "            H += chunk_matrix\n",
    "\n",
    "    if sparse:\n",
    "        assert np.allclose(H.data, H.conj().T.data), f\"The given Hamiltonian {hamiltonian} is not Hermitian: {H.data}\"\n",
    "    else:\n",
    "        assert np.allclose(H, H.conj().T), f\"The given Hamiltonian {hamiltonian} is not Hermitian: {H}\"\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell shows how qu.ham_heis works, including the convention \n",
    "# to divide interaction terms by 4 and magnetic field terms by -2\n",
    "\n",
    "H1 = qu.ham_heis(n=3, j=(1, 1, 1), sparse=False).astype(complex)\n",
    "H2 = parse_hamiltonian('0.25*(ZZI + IZZ + XXI + IXX + YYI + IYY)', dtype=complex)\n",
    "assert np.allclose(H1, H2)\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(H2.real, cmap='bwr')\n",
    "\n",
    "H1 = qu.ham_heis(n=3, j=(0, 0, 1), b=(-1, 0, 0), sparse=False).astype(complex)\n",
    "H2 = parse_hamiltonian('0.25*(ZZI + IZZ) + 0.5*(XII + IXI + IIX)', dtype=complex)\n",
    "assert np.allclose(H1, H2)\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(H2.real, cmap='bwr')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ising_model_graph(graph, J=(-1,1), h=(-1,1), g=(-1,1)):\n",
    "    \"\"\" Takes a graph and generates a Hamiltonian string for it that is compatible with `parse_hamiltonian`. \"\"\"\n",
    "    if not isinstance(graph, nk.graph.Graph):\n",
    "        raise ValueError(f\"graph must be a nk.graph.Graph, but is {type(graph)}\")\n",
    "    \n",
    "    # get the number of qubits\n",
    "    n_qubits = graph.n_nodes\n",
    "    # get the edges\n",
    "    edges = graph.edges()\n",
    "    # get the coupling matrix\n",
    "    J = np.array(J)\n",
    "    if J.shape == ():\n",
    "        # triangular matrix with all couplings set to J\n",
    "        J = np.triu(np.ones((n_qubits, n_qubits)), k=1) * J\n",
    "    elif J.shape == (2,):\n",
    "        # triangular matrix with all couplings set to a random value in this range\n",
    "        J = np.triu(np.random.uniform(J[0], J[1], (n_qubits, n_qubits)), k=1)\n",
    "    elif J.shape == (n_qubits, n_qubits):\n",
    "        # use the given matrix\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"J must be a scalar, 2-element vector, or matrix of shape {(n_qubits, n_qubits)}, but is {J.shape}\")\n",
    "    \n",
    "    # get the longitudinal fields\n",
    "    if h is not None:\n",
    "        h = np.array(h)\n",
    "        if h.shape == ():\n",
    "            h = np.ones(n_qubits) * h\n",
    "        elif h.shape == (2,):\n",
    "            h = np.random.uniform(h[0], h[1], n_qubits)\n",
    "        elif h.shape == (n_qubits,):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"h must be a scalar, 2-element vector, or vector of shape {(n_qubits,)}, but is {h.shape}\")\n",
    "        \n",
    "    # get the transverse fields\n",
    "    if g is not None:\n",
    "        g = np.array(g)\n",
    "        if g.shape == ():\n",
    "            g = np.ones(n_qubits) * g\n",
    "        elif g.shape == (2,):\n",
    "            g = np.random.uniform(g[0], g[1], n_qubits)\n",
    "        elif g.shape == (n_qubits,):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"g must be a scalar, 2-element vector, or vector of shape {(n_qubits,)}, but is {g.shape}\")\n",
    "        \n",
    "    # generate the Hamiltonian\n",
    "    H_str = ''\n",
    "    # pairwise interactions\n",
    "    for i, j in edges:\n",
    "        assert i < j, f\"edges must be sorted, but ({i}, {j}) is not\"\n",
    "        if J[i,j] != 0:\n",
    "            H_str += str(J[i,j]) + '*' + 'I'*i + 'Z' + 'I'*(j-i-1) + 'Z' + 'I'*(n_qubits-j-1) + ' + '\n",
    "    # local longitudinal fields\n",
    "    if np.any(h):\n",
    "        H_str += ' + '.join([str(h[i]) + '*' + 'I'*i + 'Z' + 'I'*(n_qubits-i-1) for i in range(n_qubits) if h[i] != 0]) + ' + '\n",
    "    # local transverse fields\n",
    "    if np.any(g):\n",
    "        H_str += ' + '.join([str(g[i]) + '*' + 'I'*i + 'X' + 'I'*(n_qubits-i-1) for i in range(n_qubits) if g[i] != 0]) + ' + '\n",
    "\n",
    "    # remove trailing ' + '\n",
    "    H_str = H_str[:-3]\n",
    "\n",
    "    return H_str\n",
    "\n",
    "def edges_from_graph(graph, undirected=False):\n",
    "    edges = graph.edges()\n",
    "    edges = np.array(edges).T\n",
    "    if undirected:\n",
    "        edges = np.concatenate([edges, edges[:,::-1]], axis=0)\n",
    "    edges = np.unique(edges, axis=0)  # sorts the edges\n",
    "    return edges[0], edges[1]\n",
    "\n",
    "def random_ising_own(N: int, graph: nk.graph.Grid):\n",
    "    \"\"\" Generates N random ising models on the given graph. \"\"\"\n",
    "    n = graph.n_nodes\n",
    "    J = np.random.uniform(-1, 1, size=(N, n, n))\n",
    "    # # make sure each J is symmetric\n",
    "    # for i in range(N):\n",
    "    #     J[i] = (J[i] + J[i].T)/2\n",
    "    #     # make sure the diagonal is zero\n",
    "    #     J[i] -= np.diag(np.diag(J[i]))\n",
    "    h = np.random.uniform(-1, 1, size=(N, n))\n",
    "    g = np.random.uniform(-1, 1, size=(N, n))\n",
    "\n",
    "    # get the edges for the coupling matrix\n",
    "    edges = edges_from_graph(graph)\n",
    "\n",
    "    hamiltonians = []\n",
    "    for i in range(N):\n",
    "        H_ising_str = ising_model_graph(graph, J[i], h[i], g[i])\n",
    "\n",
    "        # create the coupling matrix\n",
    "        J_i = np.zeros((n, n))\n",
    "        J_i[edges] = J[i][edges]\n",
    "        # convert to triu list\n",
    "        # J_i = J_i[np.triu_indices(n, k=1)]\n",
    "    \n",
    "        hamiltonians.append((H_ising_str, graph.extent, {\"J\": J_i, \"h\": h[i], \"g\": g[i]}))\n",
    "\n",
    "    return hamiltonians\n",
    "\n",
    "def random_ising_nk(N: int, graph: nk.graph.Grid):\n",
    "    J = np.random.uniform(-1, 1, size=N) # Coupling constant\n",
    "    g = np.random.uniform(-1, 1, size=N) # Transverse field\n",
    "    # J = 10*np.ones(N) # Coupling constant\n",
    "    # g = -5*np.ones(N) # Transverse field\n",
    "    n = graph.n_nodes\n",
    "    edges = edges_from_graph(graph)\n",
    "    hilbert = nk.hilbert.Spin(s=0.5, N=n)\n",
    "    h = np.zeros(n)\n",
    "    n_ones = np.ones(n)\n",
    "\n",
    "    hamiltonians = []\n",
    "    for i in range(N):\n",
    "        ising = nk.operator.Ising(\n",
    "            hilbert=hilbert,\n",
    "            graph=graph,\n",
    "            J=J[i], h=-g[i]\n",
    "        )\n",
    "\n",
    "        # Convert hyperparameters to the right format\n",
    "        J_i = np.zeros((n, n))\n",
    "        J_i[edges] = J[i]\n",
    "        # convert to triu list\n",
    "        # J_i = J_i[np.triu_indices(n, k=1)]\n",
    "    \n",
    "        hamiltonians.append((ising, graph.extent, {\"J\": J_i, \"h\": h, \"g\": g[i]*n_ones}))\n",
    "    return hamiltonians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPS with DMRG (QUIMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('../../dataset/ising/data/MPS_6000_N200_16.pt')\n",
    "data[800].grid_extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quimb as qu\n",
    "import quimb.tensor as qtn\n",
    "\n",
    "# Example setup: define these based on your data structure\n",
    "idx = 600\n",
    "n = tuple(data[idx].grid_extent)  # Number of sites, e.g., (10,)\n",
    "pbc = data[idx].pbc  # Whether periodic boundary conditions are applied\n",
    "assert not pbc, \"Periodic boundary conditions are not supported yet.\"\n",
    "\n",
    "# Initialize the Hamiltonian builder for a 1D chain with spin-1/2 (s=1/2)\n",
    "ham_builder = qtn.SpinHam1D(S=1/2)\n",
    "\n",
    "# Add single-site terms from data\n",
    "for i, (h, g) in enumerate(data[idx].x_nodes):\n",
    "    h, g = float(h), float(g)  # Ensure these are floats\n",
    "    ham_builder[i] += 2 * h, 'Z'  # Factor of 2 for the field strength, adjust as necessary\n",
    "    ham_builder[i] += 2 * g, 'X'  # Factor of 2 for the field strength, adjust as necessary\n",
    "\n",
    "# Add interaction terms between sites considering PBC\n",
    "for (a, b), J_ab in zip(data[idx].edge_index.T, data[idx].x_edges):\n",
    "    J_ab = float(J_ab)  # Ensure this is a float\n",
    "    if J_ab != 0 and (abs(a - b) == 1 or (pbc and (abs(a - b) == n[0] - 1))):\n",
    "        ham_builder[a, b] += 4 * J_ab, 'Z', 'Z'  # Add interaction terms, factor of 4 for scaling\n",
    "        ham_builder[b, a] += 4 * J_ab, 'Z', 'Z'  # Add interaction terms, factor of 4 for scaling\n",
    "\n",
    "# Build the MPO for the Hamiltonian\n",
    "H_mpo = ham_builder.build_mpo(n[0])\n",
    "\n",
    "# Set up DMRG using the MPO\n",
    "dmrg = qtn.DMRG2(H_mpo)\n",
    "dmrg.opts['max_bond'] = 30  # Adjust the maximum bond dimension as needed\n",
    "dmrg.opts['cutoff'] = 1e-10  # Set the truncation cutoff\n",
    "dmrg.opts['tol' ] = 1e-6  # Set the tolerance for convergence\n",
    "dmrg.solve(tol = 1e-6, verbosity=1)  # Perform optimization with a given number of sweeps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The ground state energy is\", dmrg.energy)\n",
    "print(\"The ground state is\", dmrg.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEPS with Simple/Full Update (QUIMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('../../dataset/ising/data/own_ham_2000_12_True.pt')\n",
    "data[800].grid_extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1400\n",
    "\n",
    "if idx is None:\n",
    "    n = (5,1)\n",
    "    graph = nk.graph.Grid(n, pbc=False)\n",
    "\n",
    "    def pos(n):\n",
    "        x,y = graph.positions[n]\n",
    "        return int(x), int(y)\n",
    "\n",
    "    H, _, hyp = random_ising_nk(1, graph)[0]\n",
    "    # construct local hamiltonians using qu.ham_heis(2, j=(0, 0, J_ab)) and qu.ham_heis(1, b=(h_a, 0, g_a))\n",
    "    local_two_site_hamiltonians = {}  # dict for qtn.LocalHam2D H2\n",
    "    for a,b in graph.edges():\n",
    "        J_ab = hyp['J'][a,b]\n",
    "        local_two_site_hamiltonians[pos(a),pos(b)] = qu.ham_heis(2, j=(0, 0, 4*J_ab))\n",
    "\n",
    "    local_one_site_hamiltonians = {}  # dict for qtn.LocalHam2D H1\n",
    "    for a in graph.nodes():\n",
    "        h_a = hyp['h'][a]\n",
    "        g_a = hyp['g'][a]\n",
    "        local_one_site_hamiltonians[pos(a)] = qu.ham_heis(1, b=(-2*g_a, 0, -2*h_a))\n",
    "else:\n",
    "    H = data[idx].hamiltonian\n",
    "    n = tuple(data[idx].grid_extent)\n",
    "    print(\"grid extent:\", n)\n",
    "\n",
    "    def pos(node):\n",
    "        # calculate x,y coordinates from node index\n",
    "        y = node % n[1]\n",
    "        x = (node - y) // n[1]\n",
    "        return x,y\n",
    "\n",
    "    # data[idx].x_nodes contains [n_nodes, 2] local field values for each node\n",
    "    local_one_site_hamiltonians = {}  # dict for qtn.LocalHam2D H1\n",
    "    for i, (h,g) in enumerate(data[idx].x_nodes):\n",
    "        h, g = float(h), float(g)  # convert from torch to float\n",
    "        local_one_site_hamiltonians[pos(i)] = qu.spin_operator('Z') * h * 2\n",
    "        local_one_site_hamiltonians[pos(i)] += qu.spin_operator('X') * g * 2\n",
    "\n",
    "    # data[idx].x_edges contains [n_edges, 1] coupling values for each edge\n",
    "    # and data[idx].edge_index contains [2, n_edges] indices of the nodes that the edge connects\n",
    "    local_two_site_hamiltonians = {}  # dict for qtn.LocalHam2D H2\n",
    "    for (a,b), J_ab in zip(data[idx].edge_index.T, data[idx].x_edges):\n",
    "        a, b, J_ab = int(a), int(b), float(J_ab)  # convert from torch to int/float\n",
    "        if J_ab != 0:\n",
    "            local_two_site_hamiltonians[pos(a),pos(b)] = qu.ham_heis(2, j=(0, 0, 4*J_ab))  # factor of 4 because of different convention\n",
    "\n",
    "    print(\"Data label ground energy\", float(data[idx].y_energy))\n",
    "\n",
    "if type(H) == str:\n",
    "    ham_full = parse_hamiltonian(H, sparse=True, dtype=float)\n",
    "else:\n",
    "    ham_full = H.to_sparse()\n",
    "\n",
    "energy_exact = qu.groundenergy(ham_full)\n",
    "ground_state_exact = qu.groundstate(ham_full)\n",
    "\n",
    "print(f'Exact ground state energy: {energy_exact}')\n",
    "\n",
    "ham_local = qtn.LocalHam2D(*n, H2=local_two_site_hamiltonians, H1=local_one_site_hamiltonians)\n",
    "# set ordering\n",
    "#ham_local.ordering = 'raster'\n",
    "\n",
    "psi0 = qtn.PEPS.rand(*n, bond_dim=4)\n",
    "psi0.show()\n",
    "su = qtn.SimpleUpdate(\n",
    "    psi0 = psi0,\n",
    "    ham = ham_local,\n",
    "    chi = 15,\n",
    "    compute_energy_every = None,\n",
    "    compute_energy_per_site = True,\n",
    "    keep_best = True,\n",
    "    progbar = True\n",
    ")\n",
    "for tau in [0.1, 0.01, 0.001]:\n",
    "    su.evolve(100, tau=tau)\n",
    "\n",
    "print(f'Approximated ground state energy: {(su.best[\"energy\"] * np.prod(n)):.6f}')\n",
    "\n",
    "# Continue with Full Update -> this doesn't improve the result at all, but it's really slow\n",
    "# fu = qtn.FullUpdate(\n",
    "#     psi0 = su.best['state'].copy(),\n",
    "#     ham = ham_local,\n",
    "#     chi = 8,\n",
    "#     compute_energy_every = None,\n",
    "#     compute_energy_per_site = True,\n",
    "#     keep_best = True,\n",
    "#     progbar = True\n",
    "# )\n",
    "# for tau in [0.3, 0.1, 0.03]:\n",
    "#     fu.evolve(50, tau=tau)\n",
    "#     print(f'Approximated ground state energy: {fu.best[\"energy\"]:.6f}')\n",
    "\n",
    "# plt.plot(su.its, su.energies, color='green')\n",
    "# plt.axhline(energy_exact, color='black')\n",
    "# plt.title('Simple Update Convergence')\n",
    "# plt.ylabel('Energy')\n",
    "# plt.xlabel('Iteration')\n",
    "\n",
    "su.best['energy'] *= np.prod(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 800\n",
    "\n",
    "H = data[idx].hamiltonian\n",
    "n = tuple(data[idx].grid_extent)\n",
    "if len(n) == 1:\n",
    "    n = (n[0], 1)\n",
    "print(\"grid extent:\", n)\n",
    "\n",
    "def pos(node):\n",
    "    # calculate x,y coordinates from node index\n",
    "    y = node % n[1]\n",
    "    x = (node - y) // n[1]\n",
    "    return x,y\n",
    "\n",
    "# def pos(node):\n",
    "#     return node\n",
    "\n",
    "\n",
    "# data[idx].x_nodes contains [n_nodes, 2] local field values for each node\n",
    "local_one_site_hamiltonians = {}  # dict for qtn.LocalHam2D H1\n",
    "for i, (h,g) in enumerate(data[idx].x_nodes):\n",
    "    print(pos(i))\n",
    "    h, g = float(h), float(g)  # convert from torch to float\n",
    "    local_one_site_hamiltonians[pos(i)] = qu.spin_operator('Z') * h * 2\n",
    "    local_one_site_hamiltonians[pos(i)] += qu.spin_operator('X') * g * 2\n",
    "\n",
    "# data[idx].x_edges contains [n_edges, 1] coupling values for each edge\n",
    "# and data[idx].edge_index contains [2, n_edges] indices of the nodes that the edge connects\n",
    "edges = []\n",
    "local_two_site_hamiltonians = {}  # dict for qtn.LocalHam2D H2\n",
    "for (a,b), J_ab in zip(data[idx].edge_index.T, data[idx].x_edges):\n",
    "    a, b, J_ab = int(a), int(b), float(J_ab)  # convert from torch to int/float\n",
    "\n",
    "    # if abs(a - b) == 1 or (a == 0 and b == n[0]-1) or (b == 0 and a == n[0]-1):\n",
    "    edge = (pos(a),pos(b))\n",
    "    if J_ab != 0:\n",
    "        edges.append(edge)\n",
    "        local_two_site_hamiltonians[pos(a),pos(b)] = qu.ham_heis(2, j=(0, 0, 4*J_ab))  # factor of 4 because of different convention\n",
    "\n",
    "print(edges)\n",
    "\n",
    "print(\"Data label ground energy\", float(data[idx].y_energy))\n",
    "\n",
    "print(local_one_site_hamiltonians)\n",
    "print(local_two_site_hamiltonians)\n",
    "\n",
    "if type(H) == str:\n",
    "    ham_full = parse_hamiltonian(H, sparse=True, dtype=float)\n",
    "else:\n",
    "    ham_full = H.to_sparse()\n",
    "\n",
    "energy_exact = qu.groundenergy(ham_full)\n",
    "ground_state_exact = qu.groundstate(ham_full)\n",
    "\n",
    "print(f'Exact ground state energy: {energy_exact}')\n",
    "\n",
    "ham_local = qtn.LocalHamGen(H2=local_two_site_hamiltonians, H1=local_one_site_hamiltonians)\n",
    "# set ordering\n",
    "ham_local.ordering = 'raster'\n",
    "\n",
    "\n",
    "psi0 = qtn.TN_from_edges_rand(edges, D=15, phys_dim=2)\n",
    "\n",
    "su = qtn.SimpleUpdateGen(\n",
    "    psi0 = psi0,\n",
    "    ham = ham_local,\n",
    "    compute_energy_every = None,\n",
    "    compute_energy_per_site = True,\n",
    "    keep_best = True,\n",
    "    progbar = True\n",
    ")\n",
    "for tau in [0.1, 0.01, 0.001]:\n",
    "    su.evolve(100, tau=tau)\n",
    "\n",
    "print(f'Approximated ground state energy: {(su.best[\"energy\"] * np.prod(n)):.6f}')\n",
    "\n",
    "# Continue with Full Update -> this doesn't improve the result at all, but it's really slow\n",
    "# fu = qtn.FullUpdate(\n",
    "#     psi0 = su.best['state'].copy(),\n",
    "#     ham = ham_local,\n",
    "#     chi = 8,\n",
    "#     compute_energy_every = None,\n",
    "#     compute_energy_per_site = True,\n",
    "#     keep_best = True,\n",
    "#     progbar = True\n",
    "# )\n",
    "# for tau in [0.3, 0.1, 0.03]:\n",
    "#     fu.evolve(50, tau=tau)\n",
    "#     print(f'Approximated ground state energy: {fu.best[\"energy\"]:.6f}')\n",
    "\n",
    "# plt.plot(su.its, su.energies, color='green')\n",
    "# plt.axhline(energy_exact, color='black')\n",
    "# plt.title('Simple Update Convergence')\n",
    "# plt.ylabel('Energy')\n",
    "# plt.xlabel('Iteration')\n",
    "\n",
    "su.best['energy'] *= np.prod(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 20\n",
    "\n",
    "# define any geometry here\n",
    "edges = [\n",
    "    (i, (i + 1) % L)\n",
    "    for i in range(L - 1)\n",
    "]\n",
    "\n",
    "print(edges)\n",
    "\n",
    "\n",
    "two = {\n",
    "    edge: qu.ham_heis(2).real\n",
    "    for edge in edges\n",
    "}\n",
    "\n",
    "one = {\n",
    "    i: (qu.spin_operator('Z')* -0.5 + qu.spin_operator('X') * -0.5)\n",
    "    for i in range(L)\n",
    "}\n",
    "\n",
    "\n",
    "ham = qtn.LocalHamGen(H2=two, H1=one)\n",
    "\n",
    "print(one)\n",
    "print(two)\n",
    "\n",
    "psi = qtn.TN_from_edges_rand(edges, D=15, phys_dim=2)\n",
    "\n",
    "su = qtn.SimpleUpdateGen(psi, ham, compute_energy_per_site=True, keep_best=True, progbar=True)\n",
    "su.evolve(30, tau=0.3)\n",
    "su.evolve(30, tau=0.1)\n",
    "su.evolve(30, tau=0.001)\n",
    "\n",
    "print(su.best['energy'] * L)\n",
    "su.state.compute_local_expectation_exact(ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = su.best['state']\n",
    "\n",
    "if len(data[idx].grid_extent) == 1:\n",
    "    m, n = (data[idx].grid_extent[0], 1)\n",
    "else:\n",
    "    m, n = tuple(data[idx].grid_extent)\n",
    "\n",
    "dims = [[2] * n] * m\n",
    "\n",
    "print(\"dims:\", dims)\n",
    "\n",
    "def pos(node):\n",
    "        # calculate x,y coordinates from node index\n",
    "        y = node % n\n",
    "        x = (node - y) // n\n",
    "        return x,y\n",
    "\n",
    "def compute_rdm(peps, sites, dims):\n",
    "    \"\"\"\n",
    "    Compute the RDM for a list of sites in a PEPS.\n",
    "\n",
    "    Parameters:\n",
    "    peps (PEPS): The PEPS representing the quantum state.\n",
    "    sites (List of tuples): The coordinates of the sites.\n",
    "    dims (list): The dimensions of the Hilbert space at each site.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The 2-RDM of the specified sites.\n",
    "    \"\"\"          \n",
    "    return qu.normalize(qu.partial_trace(peps, dims=dims, keep=sites))\n",
    "\n",
    "site1 = pos(0)\n",
    "site2 = pos(1)\n",
    "\n",
    "print(site1, site2)\n",
    "\n",
    "psi_dense = psi.to_dense()\n",
    "\n",
    "\n",
    "for i in range(len(data[idx].y_node_rdms)):\n",
    "    one_rdm = compute_rdm(psi_dense, [pos(i)], dims)\n",
    "    error = np.linalg.norm(one_rdm - data[idx].y_node_rdms[i])\n",
    "    print(f\"1RDM {i}\\t{error:.4f}\")\n",
    "\n",
    "for x in range(len(data[idx].y_edge_rdms)):\n",
    "    i, j = data[idx].edge_index[:, x]\n",
    "    two_rdm = compute_rdm(psi_dense, [pos(i), pos(j)], dims)\n",
    "    error = np.linalg.norm(two_rdm - data[idx].y_edge_rdms[x])\n",
    "    if error > 1e-6:\n",
    "        print(f\"2RDM {int(i), int(j)}\\terror: {error:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\t2RDM {int(i), int(j)}\\t*check*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainRDMs(data_point, ground_state):\n",
    "    one_rdms = []\n",
    "    two_rdms = []\n",
    "\n",
    "    m, n = tuple(data[idx].grid_extent)\n",
    "    dims = [[2] * n] * m\n",
    "\n",
    "    ground_state = ground_state.to_dense()\n",
    "\n",
    "    for node_idx in range(data_point.x_nodes.shape[0]):\n",
    "        one_rdms.append(compute_rdm(ground_state, [pos(node_idx)], dims))\n",
    "\n",
    "    for edge_idx in range(data_point.edge_index.shape[1]):\n",
    "        edge = data_point.edge_index[:, edge_idx]\n",
    "        two_rdms.append(compute_rdm(ground_state, [pos(edge[0]), pos(edge[1])], dims))\n",
    "    \n",
    "    return one_rdms, two_rdms\n",
    "\n",
    "one_rdms, two_rdms = obtainRDMs(data[idx], psi)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPS with Two-Site DMRG (TeNPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('../../dataset/ising/data/own_ham_2000_12_True.pt')\n",
    "idx = 11\n",
    "print(\"grid extent:\", data[idx].grid_extent)\n",
    "print(\"pbc:\", data[idx].pbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.load('../../dataset/ising/data/own_ham_mpsobc_10000_12_True.pt')\n",
    "# idx = 60\n",
    "# print(data[idx].grid_extent)\n",
    "# print(\"pbc:\", data[idx].pbc)\n",
    "\n",
    "# print(data[60].grid_extent)\n",
    "# print(data[3000].grid_extent)\n",
    "# print(data[4000].grid_extent)\n",
    "# print(data[6000].grid_extent)\n",
    "# print(data[8000].grid_extent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tenpy\n",
    "from tenpy.networks.site import SpinHalfSite\n",
    "from tenpy.models.tf_ising import TFIChain\n",
    "from tenpy.models.model import CouplingMPOModel\n",
    "from tenpy.models.lattice import Chain\n",
    "from tenpy.networks.mps import MPS\n",
    "from tenpy.algorithms import dmrg\n",
    "\n",
    "class Custom(CouplingMPOModel):\n",
    "    default_lattice = \"Chain\"\n",
    "    force_default_lattice = False\n",
    "    \n",
    "\n",
    "    def init_sites(self, model_param):\n",
    "        site = SpinHalfSite(conserve=None)\n",
    "        return site\n",
    "\n",
    "    def init_lattice(self, model_param):\n",
    "        sites = self.init_sites(model_param)\n",
    "        bc = \"open\" if model_param[\"bc_MPS\"] == \"open\" else \"periodic\"\n",
    "        lat = Chain(model_param[\"L\"], sites, bc=bc, bc_MPS=model_param[\"bc_MPS\"])\n",
    "        self.L = lat.N_sites\n",
    "        return lat\n",
    "\n",
    "    def init_terms(self, model_params):\n",
    "        # Add local field terms\n",
    "        for i, (h, g) in local_fields:\n",
    "            self.add_onsite_term(-h, i, 'Sigmaz')\n",
    "            self.add_onsite_term(-g, i, 'Sigmax')\n",
    "\n",
    "        # Add coupling terms\n",
    "        for (i, j), J in couplings:\n",
    "            if i > j:\n",
    "                t = i\n",
    "                i = j\n",
    "                j = t\n",
    "    \n",
    "            if J != 0:\n",
    "                self.add_coupling_term(float(J), int(i), int(j),  'Sigmaz', 'Sigmaz')\n",
    "\n",
    "\n",
    "\n",
    "print(data[idx].grid_extent)\n",
    "\n",
    "# Extract data from your dataset\n",
    "local_fields = [(i, (float(h), float(g))) for i, (h, g) in enumerate(data[idx].x_nodes)]\n",
    "couplings = [((int(a), int(b)), float(J_ab)) for (a, b), J_ab in zip(data[idx].edge_index.T, data[idx].x_edges)]\n",
    "\n",
    "# Define model parameters\n",
    "model_params = {\n",
    "    'L': int(data[idx].grid_extent[0]),\n",
    "    'local_fields': local_fields,\n",
    "    'couplings': couplings,\n",
    "    'conserve': None,\n",
    "    'bc_MPS': 'infinite' if data[idx].pbc else 'finite'\n",
    "    }\n",
    "\n",
    "bc = 'periodic' if data[idx].pbc else 'open'\n",
    "\n",
    "# Now create the model using the lattice and model_params\n",
    "model = Custom(model_params)\n",
    "\n",
    "# Initialize MPS\n",
    "psi0 = MPS.from_product_state(model.lat.mps_sites(), [\"up\"] * model.lat.N_sites, bc=model_params[\"bc_MPS\"])\n",
    "\n",
    "# DMRG parameters\n",
    "dmrg_params = {\n",
    "    'mixer': True,\n",
    "    'max_E_err': 1.e-10,\n",
    "    'trunc_params': {\n",
    "        'chi_max': 30,\n",
    "        'svd_min': 1.e-10\n",
    "    },\n",
    "    'combine': True,\n",
    "}\n",
    "\n",
    "# Run DMRG\n",
    "eng = dmrg.TwoSiteDMRGEngine(psi0, model, dmrg_params)\n",
    "E, psi = eng.run()\n",
    "\n",
    "\n",
    "print(f\"Data label ground energy {(float(data[idx].y_energy)):.6f}\")\n",
    "if data[idx].pbc:\n",
    "    print(f'Approximated ground state energy: {(E * model.lat.N_sites):.6f}')\n",
    "else:\n",
    "    print(f'Approximated ground state energy: {E:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding 1 and 2 RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(data, idx, i, j):\n",
    "    \"\"\"\n",
    "    Find indices in data[idx].edge_index where the first row equals i and the second row equals j.\n",
    "\n",
    "    Args:\n",
    "    data: The data containing edge_index tensors.\n",
    "    idx (int): The index of the specific data item.\n",
    "    i, j (int): The values to match in the first and second rows of edge_index.\n",
    "\n",
    "    Returns:\n",
    "    int: The indices where the first row equals i and the second row equals j.\n",
    "    \"\"\"\n",
    "    # Ensure i and j are within the bounds of the tensor\n",
    "    if i >= data[idx].edge_index.shape[0] or j >= data[idx].edge_index.shape[1]:\n",
    "        raise ValueError(\"i or j is out of bounds.\")\n",
    "\n",
    "    matching_indices = ((data[idx].edge_index[0] == i) & (data[idx].edge_index[1] == j)).nonzero(as_tuple=True)\n",
    "\n",
    "    print(i, j, matching_indices)\n",
    "    return matching_indices[0].item()  \n",
    "\n",
    "\n",
    "def compute_RDM_TeNPy(psi, indices):\n",
    "    \n",
    "    one_rdm = len(indices) == 1\n",
    "    two_rdm = len(indices) == 2\n",
    "\n",
    "    if not one_rdm and not two_rdm:\n",
    "        raise ValueError(\"The 'indices' argument must be a tuple of one or two integers.\")\n",
    "    \n",
    "    if one_rdm:\n",
    "        rdm = psi.get_rho_segment(indices)\n",
    "        rdm = rdm.to_ndarray()\n",
    "        rdm = rdm[::-1,::-1]\n",
    "        off = np.eye(rdm.shape[0]) != 1\n",
    "        rdm[off] *= -1\n",
    "    elif two_rdm:\n",
    "        rdm = psi.get_rho_segment(indices)\n",
    "        rdm = rdm.to_ndarray()\n",
    "        rdm = rdm.reshape(4, 4)\n",
    "        rdm = rdm[::-1, ::-1]\n",
    "        off = np.logical_and(np.eye(4) != 1, np.eye(4)[::-1] != 1)\n",
    "        rdm[off] *= -1\n",
    "        # if indices are not adjacent, we need to transform the 2-RDM\n",
    "        def swap(a, i, j):\n",
    "            a[i], a[j] = a[j], a[i]\n",
    "        if abs(indices[0] - indices[1]) != 1:\n",
    "            # swap indices to match the label\n",
    "            swap(rdm, (0,2), (1,0))\n",
    "            swap(rdm, (0,3), (1,1))\n",
    "            swap(rdm, (2,2), (3,0))\n",
    "            swap(rdm, (2,3), (3,1))\n",
    "\n",
    "    return rdm\n",
    "\n",
    "#Assert all RDMs are close to the label for each particle running compute rdm for all indices\n",
    "for i in range(len(data[idx].y_node_rdms)):\n",
    "    one_rdm = compute_RDM_TeNPy(psi, [i])\n",
    "    assert np.allclose(one_rdm, data[idx].y_node_rdms[i]), f\"1RDM {i} is not close to the label\"\n",
    "print(\"All 1RDMs are close to the label\")\n",
    "\n",
    "for x in range(len(data[idx].y_edge_rdms)):\n",
    "    i, j = data[idx].edge_index[:, x]\n",
    "    two_rdm = compute_RDM_TeNPy(psi, [i, j])\n",
    "    error = np.linalg.norm(two_rdm - data[idx].y_edge_rdms[x])\n",
    "    assert np.allclose(two_rdm, data[idx].y_edge_rdms[x]), f\"2RDM {int(i), int(j)} is not close to the label\"\n",
    "print(\"All 2RDMs are close to the label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate all 1 and 2 RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainRDMs(data_point, ground_state):\n",
    "    one_rdms = []\n",
    "    two_rdms = []\n",
    "\n",
    "\n",
    "    for node_idx in range(data_point.x_nodes.shape[0]):\n",
    "        one_rdms.append(compute_RDM_TeNPy(ground_state, [node_idx]))\n",
    "\n",
    "    for edge_idx in range(data_point.edge_index.shape[1]):\n",
    "        edge = data_point.edge_index[:, edge_idx]\n",
    "        two_rdms.append(compute_RDM_TeNPy(ground_state, [edge[0], edge[1]]))\n",
    "    \n",
    "    return one_rdms, two_rdms\n",
    "\n",
    "one_rdms, two_rdms = obtainRDMs(data[idx], psi)\n",
    "print(\"1RDMs\")\n",
    "for i in range(len(one_rdms)):\n",
    "    print(f\"{i}: {np.allclose(one_rdms[i], data[idx].y_node_rdms[i])}\")\n",
    "print(\"2RDMs\")\n",
    "for i in range(len(two_rdms)):\n",
    "    print(f\"{i}: {np.allclose(two_rdms[i], data[idx].y_edge_rdms[i])}\")\n",
    "\n",
    "\n",
    "for i in range(len(data[idx].y_node_rdms)):\n",
    "    error = np.linalg.norm(one_rdms[i] - data[idx].y_node_rdms[i])\n",
    "    print(f\"1RDM {i}\\t{error:.4f}\")\n",
    "for x in range(len(data[idx].y_edge_rdms)):\n",
    "    error = np.linalg.norm(two_rdms[x] - data[idx].y_edge_rdms[x])\n",
    "    if error > 1e-6:\n",
    "        print(f\"2RDM {int(i), int(j)}\\terror: {error:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\t2RDM {int(i), int(j)}\\t*check*\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import random\n",
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import quimb.tensor as qtn\n",
    "import quimb as qu\n",
    "\n",
    "import tenpy\n",
    "from tenpy.networks.site import SpinHalfSite\n",
    "from tenpy.models.tf_ising import TFIChain\n",
    "from tenpy.models.model import CouplingMPOModel\n",
    "from tenpy.models.lattice import Chain\n",
    "from tenpy.networks.mps import MPS\n",
    "from tenpy.algorithms import dmrg\n",
    "\n",
    "USE_FUNC = True\n",
    "\n",
    "class TensorNetworkAlgorithm(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, params):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def set_datapoint(self, data_point):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def create_model(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def run(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def getEnergy(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def getRDMs(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def getGroundState(self):\n",
    "        pass\n",
    "    @staticmethod\n",
    "    def isMPS(data_point):\n",
    "        return len(data_point.grid_extent) == 1 or data_point.grid_extent[1] == 1\n",
    "\n",
    "class SimpleUpdate(TensorNetworkAlgorithm):\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.chi = params['chi']\n",
    "        self.bond_dim = params['bond_dim']\n",
    "        self.num_iters = params['num_iters']\n",
    "        self.tau = params['tau']\n",
    "        self.psi = None\n",
    "        self.hamiltonian = None\n",
    "        self.psi = None\n",
    "        self.energy = None\n",
    "        self.data_point = None\n",
    "        self.psi0 = None\n",
    "        self.edges = []\n",
    "\n",
    "    @staticmethod\n",
    "    def pos(node, n):\n",
    "        # calculate x,y coordinates from node index\n",
    "        y = node % n[1]\n",
    "        x = (node - y) // n[1]\n",
    "        return x,y\n",
    "\n",
    "    def set_datapoint(self, data_point):\n",
    "        self.n = tuple(data_point.grid_extent)\n",
    "        self.data_point = data_point\n",
    "        self.psi0 = qtn.PEPS.rand(*self.n, bond_dim=self.bond_dim)\n",
    "                \n",
    "    def create_model(self):\n",
    "\n",
    "        local_one_site_hamiltonians = {}  # dict for qtn.LocalHam2D H1\n",
    "        for i, (h,g) in enumerate(self.data_point.x_nodes):\n",
    "            h, g = float(h), float(g)  # convert from torch to float\n",
    "            local_one_site_hamiltonians[SimpleUpdate.pos(i, self.n)] = qu.spin_operator('Z') * h * 2\n",
    "            local_one_site_hamiltonians[SimpleUpdate.pos(i, self.n)] += qu.spin_operator('X') * g * 2\n",
    "\n",
    "        local_two_site_hamiltonians = {}  # dict for qtn.LocalHam2D H2\n",
    "        for (a,b), J_ab in zip(self.data_point.edge_index.T, self.data_point.x_edges):\n",
    "            a, b, J_ab = int(a), int(b), float(J_ab)  # convert from torch to int/float\n",
    "            if J_ab != 0:\n",
    "                local_two_site_hamiltonians[SimpleUpdate.pos(a, self.n), SimpleUpdate.pos(b, self.n)] = qu.ham_heis(2, j=(0, 0, 4*J_ab))  # factor of 4 because of different convention\n",
    "\n",
    "        ham_local = qtn.LocalHam2D(*self.n, H2=local_two_site_hamiltonians, H1=local_one_site_hamiltonians)\n",
    "        self.hamiltonian = ham_local\n",
    "    \n",
    "    def run(self):\n",
    "        su = qtn.SimpleUpdate(\n",
    "            psi0 = self.psi0,\n",
    "            ham = self.hamiltonian,\n",
    "            chi = self.chi,\n",
    "            compute_energy_every = None,\n",
    "            compute_energy_per_site = True,\n",
    "            keep_best = True,\n",
    "            progbar = True\n",
    "        )\n",
    "        for tau in self.tau:\n",
    "            su.evolve(self.num_iters, tau=tau)\n",
    "        self.psi = su.best['state']\n",
    "        self.energy = su.best['energy'] * np.prod(self.n)\n",
    "    \n",
    "    def getEnergy(self):\n",
    "        return self.energy\n",
    "    \n",
    "    def getRDMs(self):\n",
    "        one_rdms = []\n",
    "        two_rdms = []\n",
    "\n",
    "        m, n = self.n\n",
    "        dims = [[2] * n] * m\n",
    "            \n",
    "\n",
    "        def compute_rdm(peps, sites, dims):\n",
    "            \"\"\"\n",
    "            Compute the RDM for a list of sites in a PEPS.\n",
    "\n",
    "            Parameters:\n",
    "            peps (PEPS): The PEPS representing the quantum state.\n",
    "            sites (List of tuples): The coordinates of the sites.\n",
    "            dims (list): The dimensions of the Hilbert space at each site.\n",
    "\n",
    "            Returns:\n",
    "            numpy.ndarray: The 2-RDM of the specified sites.\n",
    "            \"\"\"          \n",
    "            return qu.normalize(qu.partial_trace(peps, dims=dims, keep=sites))\n",
    "\n",
    "        ground_state = self.psi.to_dense()\n",
    "\n",
    "        for node_idx in range(self.data_point.x_nodes.shape[0]):\n",
    "            one_rdms.append(compute_rdm(ground_state, [SimpleUpdate.pos(node_idx, self.n)], dims))\n",
    "\n",
    "        for edge_idx in range(self.data_point.edge_index.shape[1]):\n",
    "            edge = self.data_point.edge_index[:, edge_idx]\n",
    "            two_rdms.append(compute_rdm(ground_state, [SimpleUpdate.pos(edge[0], self.n), SimpleUpdate.pos(edge[1], self.n)], dims))\n",
    "        \n",
    "        return one_rdms, two_rdms\n",
    "    \n",
    "    def getGroundState(self):\n",
    "        return self.psi\n",
    "    \n",
    "class SimpleUpdateGen(SimpleUpdate):\n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        self.edges = []\n",
    "        self.max_bond = params['max_bond']\n",
    "\n",
    "\n",
    "    def set_datapoint(self, data_point):\n",
    "        self.n = tuple(data_point.grid_extent)\n",
    "        if len(self.n) == 1:\n",
    "            self.n = (self.n[0], 1)\n",
    "        self.data_point = data_point\n",
    "        self.edges = []\n",
    "        for (a,b), J_ab in zip(self.data_point.edge_index.T, self.data_point.x_edges):\n",
    "            a, b, J_ab = int(a), int(b), float(J_ab)\n",
    "            if J_ab != 0:\n",
    "                self.edges.append((a, b))\n",
    "        self.psi0 = qtn.TN_from_edges_rand(self.edges, D=self.bond_dim, phys_dim=2)\n",
    "\n",
    "    def run(self):\n",
    "        su = qtn.SimpleUpdateGen(\n",
    "            psi0 = self.psi0,\n",
    "            ham = self.hamiltonian,\n",
    "            compute_energy_every = None,\n",
    "            compute_energy_per_site = True,\n",
    "            keep_best = True,\n",
    "            progbar = True\n",
    "        )\n",
    "\n",
    "        for tau in self.tau:\n",
    "            su.evolve(self.num_iters, tau=tau)\n",
    "\n",
    "        self.psi = su.best['state']\n",
    "        self.energy = su.best['energy'] * np.prod(self.n)\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        local_one_site_hamiltonians = {}  # dict for qtn.LocalHam2D H1\n",
    "        for i, (h,g) in enumerate(self.data_point.x_nodes):\n",
    "            h, g = float(h), float(g)  # convert from torch to float\n",
    "            local_one_site_hamiltonians[i] = qu.spin_operator('Z') * h * 2\n",
    "            local_one_site_hamiltonians[i] += qu.spin_operator('X') * g * 2\n",
    "\n",
    "        local_two_site_hamiltonians = {}  # dict for qtn.LocalHam2D H2\n",
    "        for (a,b), J_ab in zip(self.data_point.edge_index.T, self.data_point.x_edges):\n",
    "            a, b, J_ab = int(a), int(b), float(J_ab)  # convert from torch to int/float\n",
    "            if J_ab != 0:\n",
    "                local_two_site_hamiltonians[a, b] = qu.ham_heis(2, j=(0, 0, 4*J_ab))  # factor of 4 because of different convention\n",
    "\n",
    "        ham_local = qtn.LocalHamGen(H2=local_two_site_hamiltonians, H1=local_one_site_hamiltonians)\n",
    "        self.hamiltonian = ham_local\n",
    "\n",
    "    def getRDMs(self):\n",
    "        one_rdms = []\n",
    "        two_rdms = []\n",
    "\n",
    "        m, n = self.n\n",
    "        dims = [[2] * n] * m\n",
    "        \n",
    "        def pos(node, n):\n",
    "            # calculate x,y coordinates from node index\n",
    "            y = node % n[1]\n",
    "            x = (node - y) // n[1]\n",
    "            # return x, y\n",
    "            return node\n",
    "\n",
    "\n",
    "        for node_idx in range(self.data_point.x_nodes.shape[0]):\n",
    "            rdm = self.psi.partial_trace([pos(node_idx, self.n)], self.max_bond, \"auto\", normalized=True)\n",
    "            rdm.draw()\n",
    "            one_rdms.append(rdm)\n",
    "\n",
    "        for edge_idx in range(self.data_point.edge_index.shape[1]):\n",
    "            edge = self.data_point.edge_index[:, edge_idx]\n",
    "            rdm = self.psi.partial_trace([pos(edge[0], self.n), pos(edge[1], self.n)], self.max_bond, \"auto\", normalized=True)\n",
    "            rdm.draw()\n",
    "            two_rdms.append(rdm)\n",
    "        \n",
    "        return one_rdms, two_rdms\n",
    "\n",
    "class FullUpdate(SimpleUpdate):\n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        self.is_gpu = platform.system() != 'Windows'\n",
    "\n",
    "    def run(self):\n",
    "        super().run()\n",
    "\n",
    "        self.psi0 = self.psi\n",
    "\n",
    "        if self.is_gpu:\n",
    "            def to_backend(x):\n",
    "                import cupy as cp\n",
    "                return cp.asarray(x).astype('float32')\n",
    "            self.hamiltonian.apply_to_arrays(to_backend)\n",
    "            self.psi0.apply_to_arrays(to_backend)\n",
    "\n",
    "        fu = qtn.FullUpdate(\n",
    "            psi0 = self.psi0,\n",
    "            ham = self.hamiltonian,\n",
    "            chi = self.chi,\n",
    "            compute_energy_every = None,\n",
    "            compute_energy_per_site = True,\n",
    "            keep_best = True,\n",
    "            progbar = True\n",
    "        )\n",
    "        for tau in self.tau:\n",
    "            fu.evolve(self.num_iters, tau=tau)\n",
    "        self.psi = fu.best['state']\n",
    "        self.energy = fu.best['energy'] * np.prod(self.n)\n",
    "\n",
    "class CustomIsingMPOModel(CouplingMPOModel):\n",
    "    default_lattice = \"Chain\"\n",
    "    force_default_lattice = False\n",
    "    \n",
    "\n",
    "    def init_sites(self, model_param):\n",
    "        site = SpinHalfSite(conserve=None)\n",
    "        return site\n",
    "\n",
    "    def init_lattice(self, model_param):\n",
    "        sites = self.init_sites(model_param)\n",
    "        bc = \"open\" if model_param[\"bc_MPS\"] == \"open\" else \"periodic\"\n",
    "        lat = Chain(model_param[\"L\"], sites, bc=bc, bc_MPS=model_param[\"bc_MPS\"])\n",
    "        self.L = lat.N_sites\n",
    "        return lat\n",
    "\n",
    "    def init_terms(self, model_params):\n",
    "        # Add local field terms\n",
    "        for i, (h, g) in model_params[\"local_fields\"]:\n",
    "            self.add_onsite_term(-h, i, 'Sigmaz')\n",
    "            self.add_onsite_term(-g, i, 'Sigmax')\n",
    "\n",
    "        # Add coupling terms\n",
    "        for (i, j), J in model_params[\"couplings\"]:\n",
    "            if i > j:\n",
    "                t = i\n",
    "                i = j\n",
    "                j = t\n",
    "    \n",
    "            if J != 0:\n",
    "                self.add_coupling_term(float(J), int(i), int(j),  'Sigmaz', 'Sigmaz')\n",
    "\n",
    "class DMRG(TensorNetworkAlgorithm):\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.max_E_err = params['max_E_err']\n",
    "        self.chi_max = params['chi_max']\n",
    "        self.svd_min = params['svd_min']\n",
    "        self.psi = None\n",
    "        self.energy = None\n",
    "        self.data_point = None\n",
    "        self.psi0 = None\n",
    "        self.model = None\n",
    "    \n",
    "    def set_datapoint(self, data_point):\n",
    "        self.data_point = data_point\n",
    "        self.L = int(self.data_point.grid_extent[0])\n",
    "        self.bc_MPS = 'infinite' if self.data_point.pbc else 'finite'\n",
    "        self.bc = 'periodic' if self.data_point.pbc else 'open'\n",
    "        self.psi = None\n",
    "        self.energy = None\n",
    "        self.psi0 = None\n",
    "        self.model = None\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        local_fields = [(i, (float(h), float(g))) for i, (h, g) in enumerate(self.data_point.x_nodes)]\n",
    "        couplings = [((int(a), int(b)), float(J_ab)) for (a, b), J_ab in zip(self.data_point.edge_index.T, self.data_point.x_edges)]\n",
    "        \n",
    "        # Define model parameters\n",
    "        model_params = {\n",
    "            'L': self.L,\n",
    "            'local_fields': local_fields,\n",
    "            'couplings': couplings,\n",
    "            'conserve': None,\n",
    "            'bc_MPS': self.bc_MPS\n",
    "        }\n",
    "\n",
    "        self.model = CustomIsingMPOModel(model_params)\n",
    "        seq = [random.choice([\"up\", \"down\"]) for _ in range(self.model.lat.N_sites)]\n",
    "        self.psi0 = MPS.from_product_state(self.model.lat.mps_sites(), seq, bc=self.bc_MPS)\n",
    "        #self.psi0 = MPS.from_product_state(self.model.lat.mps_sites(), [\"up\"] * self.model.lat.N_sites, bc=self.bc_MPS)\n",
    "\n",
    "    def run(self):\n",
    "        dmrg_params = {\n",
    "            'mixer': True,\n",
    "            'max_E_err': self.max_E_err,\n",
    "            'trunc_params': {\n",
    "                'chi_max': self.chi_max,\n",
    "                'svd_min': self.svd_min\n",
    "            },\n",
    "            'combine': True,\n",
    "        }\n",
    "\n",
    "        if self.L > 2:\n",
    "            eng = dmrg.TwoSiteDMRGEngine(self.psi0, self.model, dmrg_params)\n",
    "        else:\n",
    "            eng = dmrg.SingleSiteDMRGEngine(self.psi0, self.model, dmrg_params)\n",
    "            \n",
    "        self.energy, self.psi = eng.run()\n",
    "        # self.energy *= self.model.lat.N_sites\n",
    "\n",
    "\n",
    "    def getEnergy(self):\n",
    "        return self.energy\n",
    "    \n",
    "    def getRDMs(self):\n",
    "        #Use TeNPy function\n",
    "        one_rdms = []\n",
    "        two_rdms = []\n",
    "\n",
    "        def compute_RDM_TeNPy(psi, indices):\n",
    "\n",
    "            one_rdm = len(indices) == 1\n",
    "            two_rdm = len(indices) == 2\n",
    "\n",
    "            if not one_rdm and not two_rdm:\n",
    "                raise ValueError(\"The 'indices' argument must be a tuple of one or two integers.\")\n",
    "            \n",
    "            if one_rdm:\n",
    "                rdm = psi.get_rho_segment(indices)\n",
    "                rdm = rdm.to_ndarray()\n",
    "                rdm = rdm[::-1,::-1]\n",
    "                off = np.eye(rdm.shape[0]) != 1\n",
    "                rdm[off] *= -1\n",
    "            elif two_rdm:\n",
    "                rdm = psi.get_rho_segment(indices)\n",
    "                rdm = rdm.to_ndarray()\n",
    "                rdm = rdm.reshape(4, 4)\n",
    "                rdm = rdm[::-1, ::-1]\n",
    "                off = np.logical_and(np.eye(4) != 1, np.eye(4)[::-1] != 1)\n",
    "                rdm[off] *= -1\n",
    "                # if indices are not adjacent, we need to transform the 2-RDM\n",
    "                def swap(a, i, j):\n",
    "                    a[i], a[j] = a[j], a[i]\n",
    "                if abs(indices[0] - indices[1]) != 1:\n",
    "                    # swap indices to match the label\n",
    "                    swap(rdm, (0,2), (1,0))\n",
    "                    swap(rdm, (0,3), (1,1))\n",
    "                    swap(rdm, (2,2), (3,0))\n",
    "                    swap(rdm, (2,3), (3,1))\n",
    "\n",
    "            return rdm\n",
    "\n",
    "        for node_idx in range(self.data_point.x_nodes.shape[0]):\n",
    "            one_rdms.append(compute_RDM_TeNPy(self.psi, [node_idx]))\n",
    "\n",
    "        for edge_idx in range(self.data_point.edge_index.shape[1]):\n",
    "            edge = self.data_point.edge_index[:, edge_idx]\n",
    "            two_rdms.append(compute_RDM_TeNPy(self.psi, [edge[0], edge[1]]))\n",
    "    \n",
    "        return one_rdms, two_rdms        \n",
    "\n",
    "    def getGroundState(self):\n",
    "        return self.psi\n",
    "    \n",
    "class DMRG_QUIMB(TensorNetworkAlgorithm):\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.max_bond = params.get('max_bond', 30)  # Maximum bond dimension\n",
    "        self.cutoff = params.get('cutoff', 1e-10)  # Truncation cutoff for SVD\n",
    "        self.tolerance = params.get('tol', 1e-6)  # Tolerance for convergence\n",
    "        self.verbosity = params.get('verbosity', 0)  # Verbosity level\n",
    "        self.psi = None  # Ground state MPS\n",
    "        self.energy = None\n",
    "        self.data_point = None\n",
    "        self.pbc = False\n",
    "\n",
    "    def set_datapoint(self, data_point):\n",
    "        if not TensorNetworkAlgorithm.isMPS(data_point):\n",
    "            raise ValueError(\"DMRG_QUIMB only supports MPS formatted data points.\")\n",
    "        self.data_point = data_point\n",
    "        self.L = data_point.grid_extent[0]  # Assuming a 1D system\n",
    "        self.pbc = data_point.pbc  # Periodic boundary conditions flag\n",
    "        assert not self.pbc, \"Periodic boundary conditions are not supported.\"\n",
    "\n",
    "    def create_model(self):\n",
    "        assert self.data_point is not None\n",
    "        # Initialize the Hamiltonian builder\n",
    "        ham_builder = qtn.SpinHam1D(S=1/2)  # Spin-1/2 Hamiltonian\n",
    "\n",
    "        # Add single-site terms\n",
    "        for i, (h, g) in enumerate(self.data_point.x_nodes):\n",
    "            h, g = float(h), float(g)\n",
    "            ham_builder[i] += 2*h, 'Z'\n",
    "            ham_builder[i] += 2*g, 'X'\n",
    "\n",
    "        # Add interaction terms, respecting PBC if necessary\n",
    "        for (a, b), J_ab in zip(self.data_point.edge_index.T, self.data_point.x_edges):\n",
    "            J_ab = float(J_ab)\n",
    "            if J_ab != 0 or (abs(a - b) == 1 or (self.pbc and (abs(a - b) == n[0] - 1))):\n",
    "                ham_builder[int(a), int(b)] +=  4*J_ab, 'Z', 'Z'\n",
    "                ham_builder[int(b), int(a)] +=  4*J_ab, 'Z', 'Z'\n",
    "\n",
    "        # Build the Hamiltonian MPO\n",
    "        self.H_mpo = ham_builder.build_mpo(self.L)\n",
    "\n",
    "    def run(self):\n",
    "        # Setup and run DMRG\n",
    "        dmrg = qtn.DMRG2(self.H_mpo)\n",
    "        dmrg.opts['max_bond'] = self.max_bond\n",
    "        dmrg.opts['cutoff'] = self.cutoff\n",
    "        dmrg.solve(tol = self.tolerance, verbosity=self.verbosity)\n",
    "        self.psi = dmrg.state\n",
    "        self.energy = dmrg.energy\n",
    "\n",
    "    def getEnergy(self):\n",
    "        return self.energy.real if self.energy else None\n",
    "\n",
    "    def getRDMs2(self):\n",
    "        one_rdms = []\n",
    "        two_rdms = []\n",
    "\n",
    "        for node_idx in range(self.data_point.x_nodes.shape[0]):\n",
    "            rdm = self.psi.partial_trace(keep=[node_idx])\n",
    "            assert len(rdm.tensors) == 1\n",
    "            one_rdms.append(qu.normalize(rdm.tensors[0].data))\n",
    "\n",
    "        for edge_idx in range(self.data_point.edge_index.shape[1]):\n",
    "            edge = self.data_point.edge_index[:, edge_idx]\n",
    "            two_rdm = self.psi.partial_trace(keep=[edge[0], edge[1]])\n",
    "            keys = list(two_rdm.ind_map.keys())\n",
    "            for ind in keys:\n",
    "                if len(two_rdm.ind_map[ind]) > 1:\n",
    "                    two_rdm.contract_ind(ind)\n",
    "            #Contract any closed legs\n",
    "            assert len(two_rdm.tensors) == 1 \n",
    "            dim = int(np.sqrt(np.prod(two_rdm.tensors[0].data.shape)))\n",
    "            two_rdms.append(qu.normalize(two_rdm.tensors[0].data.reshape(dim, dim)))\n",
    "        \n",
    "        return one_rdms, two_rdms\n",
    "    \n",
    "    def getRDMs(self):\n",
    "        one_rdms = []\n",
    "        two_rdms = []\n",
    "\n",
    "        dims = [2] * self.L\n",
    "            \n",
    "\n",
    "        def compute_rdm(peps, sites, dims):\n",
    "            \"\"\"\n",
    "            Compute the RDM for a list of sites in a PEPS.\n",
    "\n",
    "            Parameters:\n",
    "            peps (PEPS): The PEPS representing the quantum state.\n",
    "            sites (List of tuples): The coordinates of the sites.\n",
    "            dims (list): The dimensions of the Hilbert space at each site.\n",
    "\n",
    "            Returns:\n",
    "            numpy.ndarray: The 2-RDM of the specified sites.\n",
    "            \"\"\"          \n",
    "            return qu.normalize(qu.partial_trace(peps, dims=dims, keep=sites))\n",
    "\n",
    "        ground_state = self.psi.to_dense()\n",
    "\n",
    "        for node_idx in range(self.data_point.x_nodes.shape[0]):\n",
    "            one_rdms.append(compute_rdm(ground_state, [node_idx], dims))\n",
    "\n",
    "        for edge_idx in range(self.data_point.edge_index.shape[1]):\n",
    "            edge = self.data_point.edge_index[:, edge_idx]\n",
    "            two_rdms.append(compute_rdm(ground_state, [edge[0], edge[1]], dims))\n",
    "        \n",
    "        return one_rdms, two_rdms\n",
    "\n",
    "    def getGroundState(self):\n",
    "        return self.psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on all the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "#data = torch.load('..\\\\..\\\\dataset\\\\ising\\\\data\\\\PEPS_4400_N200_16.pt')\n",
    "data = torch.load('..\\\\..\\\\dataset\\\\ising\\\\data\\\\MPS_6000_N200_16.pt')\n",
    "params_SU = {\n",
    "    'chi': 15,\n",
    "    'bond_dim': 4,\n",
    "    'num_iters': 100,\n",
    "    'tau': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "params_DMRG = {\n",
    "    'max_E_err': 1.e-10,\n",
    "    'chi_max': 30,\n",
    "    'svd_min': 1.e-10\n",
    "}\n",
    "\n",
    "params_DMRG_QUIMB = {\n",
    "    'max_bond': 60,\n",
    "    'cutoff': 1.e-10,\n",
    "    'tol': 1.e-6,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "params_SU_gen = {\n",
    "    'chi': 30,\n",
    "    'max_bond':30,\n",
    "    'bond_dim': 2,\n",
    "    'num_iters': 100,\n",
    "    'tau': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "def print_and_save(output):\n",
    "    file = \"results_aux.txt\"\n",
    "    print(output)\n",
    "    with open(file, \"a\") as file_object:\n",
    "        file_object.write(output + '\\n')    \n",
    "\n",
    "def matrix_sqrt(A):\n",
    "    \"\"\"Compute the square root of a positive semi-definite matrix, assuming A is Hermitian.\"\"\"\n",
    "    vals, vecs = torch.linalg.eigh(A)\n",
    "    sqrt_vals = torch.sqrt(torch.clamp(vals, min=0))  # Ensure eigenvalues are non-negative\n",
    "    sqrt_vals_complex = torch.diag_embed(sqrt_vals).to(dtype=torch.complex128)  # Use complex type\n",
    "    return vecs @ sqrt_vals_complex @ vecs.conj().transpose(-2, -1)\n",
    "\n",
    "def fidelity_torch(rho, sigma):\n",
    "    \"\"\"Calculate the fidelity between two density matrices rho and sigma.\"\"\"\n",
    "    # Compute the square root of rho\n",
    "    sqrt_rho = matrix_sqrt(rho)\n",
    "    \n",
    "    # Compute the product sqrt_rho * sigma * sqrt_rho\n",
    "    middle_product = sqrt_rho @ sigma @ sqrt_rho\n",
    "    \n",
    "    # Compute the square root of the middle product\n",
    "    sqrt_middle_product = matrix_sqrt(middle_product)\n",
    "    \n",
    "    # Compute the trace of the sqrt_middle_product and then square the result for fidelity\n",
    "    trace_value = torch.trace(sqrt_middle_product)\n",
    "    \n",
    "    # Return the square of the trace, since fidelity is the square of the trace of the square root of the middle product\n",
    "    return trace_value.real**2\n",
    "\n",
    "\n",
    "def isMPS(data_point):\n",
    "    return len(data_point.grid_extent) == 1 or data_point.grid_extent[1] == 1\n",
    "\n",
    "def checkEnergy(energy, label):\n",
    "    return np.allclose(energy, label, atol=1e-2)\n",
    "\n",
    "def checkRDMs(rdms, labels):\n",
    "    errors = []\n",
    "    for i in range(len(rdms)):\n",
    "        errors.append(fidelity_torch(torch.tensor(rdms[i].copy(), dtype=torch.cdouble), torch.tensor(labels[i], dtype=torch.cdouble)).numpy())\n",
    "    return np.allclose(errors, 0.1), errors\n",
    "\n",
    "#Ignore UserWarnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for idx in [200,600]:\n",
    "        data_point = data[idx]\n",
    "        print(data_point.grid_extent)\n",
    "        if isMPS(data_point):\n",
    "            print_and_save(f\"{idx}, MPS: {data_point.grid_extent}, PBC: {data_point.pbc}\")\n",
    "            # alg = DMRG(params_DMRG)\n",
    "            alg = DMRG_QUIMB(params_DMRG_QUIMB)\n",
    "        else:\n",
    "            print_and_save(f\"{idx}, PEPS: {data_point.grid_extent}, PBC: {data_point.pbc}\")\n",
    "            alg = SimpleUpdate(params_SU)\n",
    "        # alg = SimpleUpdateGen(params_SU_gen)\n",
    "        alg.set_datapoint(data_point)\n",
    "        alg.create_model()\n",
    "        alg.run()\n",
    "        correct_energy = checkEnergy(alg.getEnergy(), data_point.y_energy)\n",
    "\n",
    "        print(\"Predicted energy:\", alg.getEnergy())\n",
    "        print(\"Label energy:\", data_point.y_energy)\n",
    "        print(\"PBC: \", data_point.pbc)\n",
    "\n",
    "\n",
    "        one_rdms, two_rdms = alg.getRDMs()\n",
    "        correct_one_rdms , one_rdm_errors = checkRDMs(one_rdms, data_point.y_node_rdms)\n",
    "        correct_two_rdms , two_rdm_errors = checkRDMs(two_rdms, data_point.y_edge_rdms)\n",
    "\n",
    "        #print total distance between calculated and label RDMs\n",
    "        # print_and_save(f\"Total distance between calculated and label 1RDMs: {np.array(one_rdm_errors)}\")\n",
    "        # print_and_save(f\"Total distance between calculated and label 2RDMs: {np.array(two_rdm_errors)}\")\n",
    "        #Average distance between calculated and label RDMs\n",
    "        print_and_save(f\"Average distance between calculated and label 1RDMs: {np.mean(one_rdm_errors)}\")\n",
    "        print_and_save(f\"Average distance between calculated and label 2RDMs: {np.mean(two_rdm_errors)}\")\n",
    "        \n",
    "\n",
    "        print_and_save(f'Correct Energy: {correct_energy}, Correct 1RDMs: {correct_one_rdms}, Correct 2RDMs: {correct_two_rdms}')\n",
    "\n",
    "        # if not correct_energy or not correct_one_rdms or not correct_two_rdms:\n",
    "        #     if not correct_energy:\n",
    "        #         print_and_save(f\"Energy Error: {np.linalg.norm(alg.getEnergy() - data_point.y_energy):.6f}\")\n",
    "\n",
    "        #     if not correct_one_rdms:\n",
    "        #         pr = False\n",
    "        #         one_rdm_errors = []\n",
    "        #         for i in range(len(one_rdms)):\n",
    "        #             error = np.linalg.norm(one_rdms[i] - data_point.y_node_rdms[i])\n",
    "        #             if error > 0.05:\n",
    "        #                 pr = True\n",
    "        #                 one_rdm_errors.append(f\"[({i}):{error:.4f}]\")\n",
    "        #             print(f\"1RDM calc{i}\\t{one_rdms[i]}\")\n",
    "        #             print(f\"1RDM labl{i}\\t{data_point.y_node_rdms[i]}\")\n",
    "        #         if pr:\n",
    "        #             print_and_save(\"One RDM Errors:\")\n",
    "        #             print_and_save(\",\".join(one_rdm_errors))\n",
    "\n",
    "        #     if not correct_two_rdms:\n",
    "        #         pr = False\n",
    "        #         two_rdm_errors = []\n",
    "        #         for x in range(len(two_rdms)):\n",
    "        #             i, j = data_point.edge_index[:, x]\n",
    "        #             error = np.linalg.norm(two_rdms[x] - data_point.y_edge_rdms[x])\n",
    "        #             if error > 0.05:\n",
    "        #                 pr = True\n",
    "        #                 two_rdm_errors.append(f\"[{int(i), int(j)}:{error:.4f}]\")\n",
    "        #             print(f\"2RDM calc{i,j}\\t{two_rdms[x]}\")\n",
    "        #             print(f\"2RDM labl{i,j}\\t{data_point.y_edge_rdms[x]}\")\n",
    "        #         if pr:\n",
    "        #             print_and_save(\"Two RDM Errors:\")\n",
    "        #             print_and_save(\",\".join(two_rdm_errors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
