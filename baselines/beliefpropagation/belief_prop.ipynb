{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "\n",
    "import networkx as nx\n",
    "from factor import Factor\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, second try. This time I'm actually implementing belief propagation. Three steps now.\n",
    "\n",
    "1. Belief prop for my Factor class\n",
    "2. Belief prop for tensor networks\n",
    "3. (This is in `tn_simple_update.ipynb`!) Use the Simple- or Full-Update algorithm from `quimb` to do the belief prop (which is probably what we will use in the end)\n",
    "4. (Here again) Implement belief prop as shown in the paper `Tensor networks contraction and the belief propagation algorithm` by Alkabetz and Arad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a network to work with\n",
    "\n",
    "def construct_graph_from_edges(edges):\n",
    "    \"\"\" Constructs a graph from a list of edges. Every edge is a tuple of the form (node1, node2, size). The tensor at each node is initialized to a random tensor of the corresponding shape. \"\"\"\n",
    "\n",
    "    def get_edge_data(graph, node, sort_by=0):\n",
    "        \"\"\" Finds all edges connected to the given node and returns their corresponding data. \"\"\"\n",
    "        edges = []\n",
    "        for n in graph.neighbors(node):\n",
    "            for e, e_data in graph.get_edge_data(node, n).items():\n",
    "                edges.append(list(e_data.values()))\n",
    "        return np.unique(edges, axis=sort_by).T\n",
    "\n",
    "    graph = nx.MultiGraph()\n",
    "    for i, edge in enumerate(edges):\n",
    "        var_label = chr(ord('i') + i)\n",
    "        graph.add_edge(edge[0], edge[1], var=var_label, size=edge[2])\n",
    "\n",
    "    for n in list(graph.nodes):\n",
    "        variables, sizes = get_edge_data(graph, n)\n",
    "        variables, sizes = list(variables), [int(s) for s in sizes]\n",
    "        tensor = normalize(np.random.rand(*sizes), p=1, axis=0)\n",
    "        graph.add_node(n, factor=Factor(variables, tensor))\n",
    "\n",
    "    return graph\n",
    "\n",
    "def draw_graph(graph):\n",
    "    def get_edge_labels(graph):\n",
    "        edge_labels = {}\n",
    "        for i, j, d in graph.edges(data=True):\n",
    "            if (i,j) in edge_labels:\n",
    "                edge_labels[(i,j)] += f\", {d['var']}\"\n",
    "            else:\n",
    "                edge_labels[(i,j)] = str(d['var'])\n",
    "        for (i, j), s in edge_labels.items():\n",
    "            if i == j:\n",
    "                edge_labels[(i,j)] = \"______\" + s\n",
    "        return edge_labels\n",
    "\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(graph, pos, with_labels=True, width=[3**(graph.number_of_edges(i,j)-1) for i, j, d in graph.edges(data=True)])\n",
    "    nx.draw_networkx_edge_labels(graph, pos, edge_labels=get_edge_labels(graph))\n",
    "\n",
    "edges = [('a', 'a', 2), ('a', 'c', 2), ('b', 'c', 3), ('c', 'd', 2), ('c', 'd', 3), ('d', 'd', 2)] # each edge corresponds to a variable / index (networkx doesn't support hyperedges)\n",
    "net = construct_graph_from_edges(edges)\n",
    "draw_graph(net)\n",
    "print(\"Network:\", {n :d['factor'].variables for n, d in net.nodes(data=True)})\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph_from_factors(factors: List[Factor]):\n",
    "    \"\"\" Constructs a graph from a list of factors. Every factor is a tuple of the form (variables, tensor). \"\"\"\n",
    "    graph = nx.MultiGraph()\n",
    "    for i, factor in enumerate(factors):\n",
    "        # node_label = chr(ord('a') + i)\n",
    "        node_label = ''.join(factor.variables)\n",
    "        graph.add_node(node_label, factor=factor)\n",
    "\n",
    "    for i, factor in enumerate(factors):\n",
    "        # Find the node that contains the factor\n",
    "        node = None\n",
    "        for n, d in graph.nodes(data=True):\n",
    "            if d['factor'] == factor:\n",
    "                node = n\n",
    "                break\n",
    "        if node is None:\n",
    "                raise Exception(\"Could not find node for factor:\", factor)\n",
    "        for v in factor.variables:\n",
    "            for other_factor in factors:\n",
    "                if v in other_factor.variables and other_factor != factor:\n",
    "                    # print(f\"{''.join(factor.variables)}: Found {v} in {''.join(other_factor.variables)}\")\n",
    "                    # Find the node that contains the other factor\n",
    "                    other_node = None\n",
    "                    for n, d in graph.nodes(data=True):\n",
    "                        if d['factor'] == other_factor:\n",
    "                            other_node = n\n",
    "                            break\n",
    "                    if  other_node is None:\n",
    "                        raise Exception(\"Could not find node for factor:\", other_factor)\n",
    "                    # Add an edge between the two nodes if there is not already the opposite edge\n",
    "                    if not (graph.has_edge(node, other_node) or graph.has_edge(other_node, node)):\n",
    "                        graph.add_edge(node, other_node, var=v, size=factor.data.shape[factor.variables.index(v)])\n",
    "            # If there is no other, add a self-loop\n",
    "            if not any([v in other_factor.variables for other_factor in factors if other_factor != factor]):\n",
    "                # print(f\"{''.join(factor.variables)}: Found {v} in no other factor\")\n",
    "                graph.add_edge(node, node, var=v, size=factor.data.shape[factor.variables.index(v)])\n",
    "\n",
    "    return graph\n",
    "\n",
    "# This is the tree in Barber Figure 14.1 and 14.2\n",
    "net_barber = construct_graph_from_factors([\n",
    "    Factor('A', np.array([0.01, 0.99])),\n",
    "    Factor('AB', np.array([[0.1, 0.9], [0.001, 0.999]])),\n",
    "    Factor('C', np.array([0.001, 0.999])),\n",
    "    Factor('BCD', np.array([[[0.99, 0.01], [0.9, 0.1]], [[0.95, 0.05], [0.01, 0.99]]])),\n",
    "    Factor('DE', np.array([[0.9, 0.1], [0.3, 0.7]])),\n",
    "    Factor('DF', np.array([[0.2, 0.8], [0.1, 0.9]])),\n",
    "])\n",
    "# remove edge between 'DE' and 'DF'\n",
    "net_barber.remove_edge('DE', 'DF')\n",
    "draw_graph(net_barber)\n",
    "print(\"Network:\", {n :d['factor'].variables for n, d in net_barber.nodes(data=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an attempt to add tensors to both nodes and edges\n",
    "# tree = nx.Graph()\n",
    "\n",
    "# nodes = [(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2)]\n",
    "# edges = [(0, 1), (0, 2), (1, 3), (1, 4), (2, 5)]\n",
    "# # add a size attribute to each node\n",
    "# for node, size in nodes:\n",
    "#     tree.add_node(node, size=size, f=np.random.rand(size))\n",
    "# # add a tensor to each edge as an attribute with the size of the respective nodes\n",
    "# for i,j in edges:\n",
    "#     f = np.random.rand(tree.nodes[i]['size'], tree.nodes[j]['size'])\n",
    "#     # tensor = normalize(tensor, 1, axis=1)\n",
    "#     tree.add_edge(i, j, f=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Belief propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def belief_prop(graph, query, max_iterations=100, eps=1e-6):\n",
    "    \"\"\"\n",
    "        Computes the marginal distribution of the given query variables using the belief propagation algorithm.    \n",
    "    \n",
    "        Parameters\n",
    "            graph (nx.Graph): The graph for which the marginals should be computed.\n",
    "            Each node should have the attribute 'factor' containing a Factor object, and each edge should have \n",
    "            an attribute 'var' containing the name of the variable present the in factors connected by the edge.\n",
    "\n",
    "            query (list[str]): A list of variables for which the marginals should be computed\n",
    "\n",
    "        Returns\n",
    "            np.ndarray: An array containing the marginals for the given nodes\n",
    "    \"\"\"\n",
    "\n",
    "    # Big table of messages\n",
    "    messages = {}\n",
    "\n",
    "    # Initialize messages\n",
    "    for node in graph.nodes:\n",
    "        f = graph.nodes[node]['factor']\n",
    "        for neighbor in graph.neighbors(node):\n",
    "            # incoming messages to the node\n",
    "            messages[(neighbor, node)] = Factor(\n",
    "                potentials=np.ones(f.data.shape),\n",
    "                variables=f.variables.copy()\n",
    "            )\n",
    "\n",
    "    # Iterate until convergence\n",
    "    for i in range(max_iterations):\n",
    "        # print(\"Iteration\", i)\n",
    "        old_messages = messages.copy()\n",
    "        # Compute messages\n",
    "        for node in graph.nodes:\n",
    "            for neighbor in graph.neighbors(node):\n",
    "                # print(f\"Computing message {node} -> {neighbor}\")\n",
    "                # Aggregate messages from other neighbors\n",
    "                aggregate = Factor()\n",
    "                for other_neighbor in graph.neighbors(node):\n",
    "                    if other_neighbor != neighbor:\n",
    "                        aggregate = aggregate * messages[(other_neighbor, node)]\n",
    "                # Multiply with the marginal of the node\n",
    "                total = graph.nodes[node]['factor'] * aggregate\n",
    "                # Sum out all variables that are not in the neighbor\n",
    "                for v in total.variables:\n",
    "                    if v not in graph.nodes[neighbor]['factor'].variables:\n",
    "                        total = total.marginalize(v)\n",
    "                # print(\"total:\",total.variables, \"neighbor:\", graph.nodes[neighbor]['factor'].variables)\n",
    "                # Normalize the message\n",
    "                message = total.normalize()\n",
    "                # print(f\"New message {node} -> {neighbor}:\", message)\n",
    "                # Update the message\n",
    "                if i > 0 and (message.variables != old_messages[(node, neighbor)].variables):\n",
    "                    print(f\"Message variables {message.variables} do not match old message variables {old_messages[(node, neighbor)].variables}\")\n",
    "                messages[(node, neighbor)] = message\n",
    "        # Check for convergence\n",
    "        if i == 0:\n",
    "            continue\n",
    "        error = 0\n",
    "        for key in messages:\n",
    "            error += np.sum(np.abs(messages[key].data - old_messages[key].data))\n",
    "        # print(\"Error:\", error)\n",
    "        if error < eps:\n",
    "            print(\"Converged after\", i, \"iterations\")\n",
    "            break\n",
    "\n",
    "    # Compute marginals for the query variables\n",
    "    marginal = Factor()\n",
    "    for node in graph.nodes:\n",
    "        if any([v in graph.nodes[node]['factor'].variables for v in query]):\n",
    "            marginal = marginal * graph.nodes[node]['factor']\n",
    "    for v in marginal.variables:\n",
    "        if v not in query:\n",
    "            marginal = marginal.marginalize(v)\n",
    "\n",
    "    marginal.transpose(query, inplace=True)\n",
    "    return marginal.normalize()\n",
    "\n",
    "belief_prop(net, 'ij'), belief_prop(net_barber, 'C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Belief propagation for tensor networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quimb as qu\n",
    "import quimb.tensor as qtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tn_mul(t1, t2):\n",
    "        \"\"\" Multiply two tensors element-wise, adding new indices if necessary. New indices are initialized to 1. \"\"\"\n",
    "        def add_indices(target, like):\n",
    "            \"\"\" Add all indices of `like` to `target` if they are not already present. \"\"\"\n",
    "            for i in reversed(like.inds):\n",
    "                if i not in target.inds:\n",
    "                    target.new_ind(i, size=0, axis=-1) # size 0 is a placeholder and will be updated later automatically\n",
    "\n",
    "        t1 = t1.copy()\n",
    "        t2 = t2.copy()\n",
    "        # add all extra indices of t2 to t1 and vice versa\n",
    "        add_indices(target=t1, like=t2)\n",
    "        add_indices(target=t2, like=t1)\n",
    "        # ensure t2 indices have the same order as t1 indices\n",
    "        t2.transpose(*t1.inds, inplace=True)\n",
    "        # elementwise multiplication\n",
    "        return t1 * t2\n",
    "\n",
    "# Equivalent to `belief_prop` above, but using `quimb.tensor.TensorNetwork` instead of `networkx.Graph`\n",
    "def belief_prop_tn(tn, query, max_iterations=100, eps=1e-6):\n",
    "    \"\"\" Computes the marginals for given indices in `query`. \"\"\"\n",
    "    # Initialize messages\n",
    "    messages = {}\n",
    "    for n in tn.tensors:\n",
    "        # incoming messages to n\n",
    "        for m in tn.select_neighbors(n.tags):\n",
    "            messages[(list(n.tags)[0], list(m.tags)[0])] = qtn.Tensor(\n",
    "                data=np.ones(n.data.shape),\n",
    "                inds=n.inds,\n",
    "                tags=n.tags\n",
    "            )\n",
    "\n",
    "    # Iterate until convergence\n",
    "    for i in range(max_iterations):\n",
    "        # print(\"Iteration\", i)\n",
    "        old_messages = messages.copy()\n",
    "        # Compute messages\n",
    "        for n in tn.tensors:\n",
    "            for m in tn.select_neighbors(n.tags):\n",
    "                # print(f\"Computing message {n} -> {m}\")\n",
    "                # Aggregate messages from other neighbors\n",
    "                aggregate = qtn.Tensor(data=1, inds=[], tags=[])\n",
    "                for other_neighbor in tn.select_neighbors(n.tags):\n",
    "                    if other_neighbor != m:\n",
    "                        aggregate = tn_mul(aggregate, messages[(list(other_neighbor.tags)[0], list(n.tags)[0])])\n",
    "                # Multiply with the marginal of the node\n",
    "                total = tn_mul(n, aggregate)\n",
    "                # Sum out all variables that are not in the neighbor\n",
    "                for v in total.inds:\n",
    "                    if v not in m.inds:\n",
    "                        total = total.sum_reduce(v)\n",
    "                # print(\"total:\",total.inds, \"neighbor:\", tn[m].inds)\n",
    "                # Normalize the message\n",
    "                message = total / total.data.sum()\n",
    "                # print(f\"New message {n} -> {m}:\", message)\n",
    "                # Update the message\n",
    "                if i > 0 and (message.inds != old_messages[(list(n.tags)[0], list(m.tags)[0])].inds):\n",
    "                    print(f\"Message indices {message.inds} do not match old message indices {old_messages[(n.tags, m.tags)].inds}\")\n",
    "                messages[(list(n.tags)[0], list(m.tags)[0])] = message\n",
    "        # Check for convergence\n",
    "        if i == 0:\n",
    "            continue\n",
    "        error = 0\n",
    "        for key in messages:\n",
    "            error += np.sum(np.abs(messages[key].data - old_messages[key].data))\n",
    "        # print(\"Error:\", error)\n",
    "        if error < eps:\n",
    "            print(\"Converged after\", i, \"iterations\")\n",
    "            break\n",
    "\n",
    "    # Compute marginals for the query variables\n",
    "    marginal = qtn.Tensor(data=1, inds=[], tags=[])\n",
    "    for n in tn.tensors:\n",
    "        if any([v in n.inds for v in query]):\n",
    "            marginal = tn_mul(marginal, n)\n",
    "    for v in marginal.inds:\n",
    "        if v not in query:\n",
    "            marginal = marginal.sum_reduce(v)\n",
    "\n",
    "    marginal.transpose(*query, inplace=True)\n",
    "    return marginal / marginal.data.sum()\n",
    "\n",
    "def get_tn(graph):\n",
    "    \"\"\" Returns a tensor network from the given graph. \"\"\"\n",
    "    T = qtn.TensorNetwork()\n",
    "    for n, n_data in graph.nodes(data=True):\n",
    "        tensor = qtn.Tensor(n_data['factor'].data, inds=n_data['factor'].variables, tags=n)\n",
    "        T |= tensor\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tensor network from the graph `net`\n",
    "T = get_tn(net)\n",
    "# T.draw()\n",
    "query = 'ij'\n",
    "\n",
    "assert np.allclose(belief_prop(net, query).data, belief_prop_tn(T, query).data)\n",
    "\n",
    "belief_prop_tn(T, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_barber = get_tn(net_barber)\n",
    "query = 'C'\n",
    "assert np.allclose(belief_prop(net_barber, query).data, belief_prop_tn(T_barber, query).data)\n",
    "belief_prop_tn(T_barber, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = qtn.Tensor(np.random.rand(2, 3, 4), inds='abc', tags='t1')\n",
    "t2 = qtn.Tensor(np.random.rand(3, 4, 5), inds='bcd', tags='t2')\n",
    "tn = t1 | t2\n",
    "list(t1.tags)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Belief propagation for tensor networks 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quimb as qu\n",
    "import quimb.tensor as qtn\n",
    "\n",
    "def tn_mul(t1, t2):\n",
    "        \"\"\" Multiply two tensors element-wise, adding new indices if necessary. New indices are initialized to 1. \"\"\"\n",
    "        def add_indices(target, like):\n",
    "            \"\"\" Add all indices of `like` to `target` if they are not already present. \"\"\"\n",
    "            for i in reversed(like.inds):\n",
    "                if i not in target.inds:\n",
    "                    target.new_ind(i, size=0, axis=-1) # size 0 is a placeholder and will be updated later automatically\n",
    "\n",
    "        t1 = t1.copy()\n",
    "        t2 = t2.copy()\n",
    "        # add all extra indices of t2 to t1 and vice versa\n",
    "        add_indices(target=t1, like=t2)\n",
    "        add_indices(target=t2, like=t1)\n",
    "        # ensure t2 indices have the same order as t1 indices\n",
    "        t2.transpose(*t1.inds, inplace=True)\n",
    "        # elementwise multiplication\n",
    "        return t1 * t2\n",
    "\n",
    "# Equivalent to `belief_prop` above, but using `quimb.tensor.TensorNetwork` instead of `networkx.Graph`\n",
    "def belief_prop_tn(tn, query, max_iterations=100, eps=1e-6):\n",
    "    \"\"\" Computes the marginals for given indices in `query`. \"\"\"\n",
    "    # Initialize messages\n",
    "    messages = {}\n",
    "    for n in tn.tensors:\n",
    "        # incoming messages to n\n",
    "        for m in tn.select_neighbors(n.tags):\n",
    "            messages[(list(n.tags)[0], list(m.tags)[0])] = qtn.Tensor(\n",
    "                data=np.ones(n.data.shape),\n",
    "                inds=n.inds,\n",
    "                tags=n.tags\n",
    "            )\n",
    "\n",
    "    # Iterate until convergence\n",
    "    for i in range(max_iterations):\n",
    "        # print(\"Iteration\", i)\n",
    "        old_messages = messages.copy()\n",
    "        # Compute messages\n",
    "        for n in tn.tensors:\n",
    "            for m in tn.select_neighbors(n.tags):\n",
    "                # print(f\"Computing message {n} -> {m}\")\n",
    "                # Aggregate messages from other neighbors\n",
    "                aggregate = qtn.Tensor(data=1, inds=[], tags=[])\n",
    "                for other_neighbor in tn.select_neighbors(n.tags):\n",
    "                    if other_neighbor != m:\n",
    "                        aggregate = tn_mul(aggregate, messages[(list(other_neighbor.tags)[0], list(n.tags)[0])])\n",
    "                # Multiply with the marginal of the node\n",
    "                total = tn_mul(n, aggregate)\n",
    "                # Sum out all variables that are not in the neighbor\n",
    "                for v in total.inds:\n",
    "                    if v not in m.inds:\n",
    "                        total = total.sum_reduce(v)\n",
    "                # print(\"total:\",total.inds, \"neighbor:\", tn[m].inds)\n",
    "                # Normalize the message\n",
    "                message = total / total.data.sum()\n",
    "                # print(f\"New message {n} -> {m}:\", message)\n",
    "                # Update the message\n",
    "                if i > 0 and (message.inds != old_messages[(list(n.tags)[0], list(m.tags)[0])].inds):\n",
    "                    print(f\"Message indices {message.inds} do not match old message indices {old_messages[(n.tags, m.tags)].inds}\")\n",
    "                messages[(list(n.tags)[0], list(m.tags)[0])] = message\n",
    "        # Check for convergence\n",
    "        if i == 0:\n",
    "            continue\n",
    "        error = 0\n",
    "        for key in messages:\n",
    "            error += np.sum(np.abs(messages[key].data - old_messages[key].data))\n",
    "        # print(\"Error:\", error)\n",
    "        if error < eps:\n",
    "            print(\"Converged after\", i, \"iterations\")\n",
    "            break\n",
    "\n",
    "    # Compute marginals for the query variables\n",
    "    marginal = qtn.Tensor(data=1, inds=[], tags=[])\n",
    "    for n in tn.tensors:\n",
    "        if any([v in n.inds for v in query]):\n",
    "            marginal = tn_mul(marginal, n)\n",
    "    for v in marginal.inds:\n",
    "        if v not in query:\n",
    "            marginal = marginal.sum_reduce(v)\n",
    "\n",
    "    marginal.transpose(*query, inplace=True)\n",
    "    return marginal / marginal.data.sum()\n",
    "\n",
    "def get_tn(graph):\n",
    "    \"\"\" Returns a tensor network from the given graph. \"\"\"\n",
    "    T = qtn.TensorNetwork()\n",
    "    for n, n_data in graph.nodes(data=True):\n",
    "        tensor = qtn.Tensor(n_data['factor'].data, inds=n_data['factor'].variables, tags=n)\n",
    "        T |= tensor\n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Simple Update Algorithm with tnsu package for Hamiltonians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "sys.path.insert(1, '..\\\\..\\\\dataset\\\\ising')\n",
    "\n",
    "\n",
    "from isingModel import IsingModelDataset\n",
    "\n",
    "data_file = \"..\\\\..\\\\dataset\\\\ising\\\\data\\\\nk2_2000_12_True.pt\"\n",
    "data_file = \"..\\\\..\\\\dataset\\\\ising\\\\data\\\\nk_2000_12_True.pt\"\n",
    "# Load the dataset\n",
    "dataset = IsingModelDataset.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = int(torch.randint(len(dataset), (1,)))\n",
    "data_point = dataset[rand_idx]\n",
    "\n",
    "idx = -1\n",
    "\n",
    "for _idx, point in enumerate(dataset):\n",
    "    if len(point.x_nodes) == 4:\n",
    "        idx = _idx\n",
    "assert idx != -1\n",
    "data_point = dataset[idx]\n",
    "print(data_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_point = torch.load(\"test_point.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from functools import reduce\n",
    "\n",
    "matmap_np, matmap_sp = None, None\n",
    "\n",
    "def parse_hamiltonian(hamiltonian, sparse=False, scaling=1, buffer=None, max_buffer_n=0, dtype=float): # I'd usually default to complex, but because we're only dealing with Ising models here, float is more handy\n",
    "    \"\"\"Parse a string representation of a Hamiltonian into a matrix representation. The result is guaranteed to be Hermitian.\n",
    "\n",
    "    Parameters:\n",
    "        hamiltonian (str): The Hamiltonian to parse.\n",
    "        sparse (bool): Whether to use sparse matrices (csr_matrix) or dense matrices (numpy.array).\n",
    "        scaling (float): A constant factor to scale the Hamiltonian by.\n",
    "        buffer (dict): A dictionary to store calculated chunks in. If `None`, it defaults to the global `matmap_np` (or `matmap_sp` if `sparse == True`). Give `buffer={}` and leave `max_buffer_n == 0` (default) to disable the buffer.\n",
    "        max_buffer_n (int): The maximum length (number of qubits) for new chunks to store in the buffer (default: 0). If `0`, no new chunks will be stored in the buffer.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray | scipy.sparse.csr_matrix: The matrix representation of the Hamiltonian.\n",
    "\n",
    "    Example:\n",
    "    >>> parse_hamiltonian('0.5*(XX + YY + ZZ + II)') # SWAP\n",
    "    array([[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  1.+0.j  0.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  0.+0.j  0.+0.j  1.+0.j]])\n",
    "    >>> parse_hamiltonian('-(XX + YY + .5*ZZ) + 1.5')\n",
    "    array([[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  2.+0.j -2.+0.j  0.+0.j]\n",
    "           [ 0.+0.j -2.+0.j  2.+0.j  0.+0.j]\n",
    "           [ 0.+0.j  0.+0.j  0.+0.j  1.+0.j]])\n",
    "    >>> parse_hamiltonian('0.5*(II + ZI - ZX + IX)') # CNOT\n",
    "\n",
    "    \"\"\"\n",
    "    kron = sp.kron if sparse else np.kron\n",
    "\n",
    "    # Initialize the matrix map\n",
    "    global matmap_np, matmap_sp\n",
    "    if matmap_np is None or matmap_sp is None or matmap_np[\"I\"].dtype != dtype:\n",
    "        # numpy versions\n",
    "        matmap_np = {\n",
    "            \"H\": np.array([[1, 1], [1, -1]], dtype=dtype) / np.sqrt(2),\n",
    "            \"X\": np.array([[0, 1], [1, 0]], dtype=dtype),\n",
    "            \"Z\": np.array([[1, 0], [0, -1]], dtype=dtype),\n",
    "            \"I\": np.array([[1, 0], [0, 1]], dtype=dtype),\n",
    "        }\n",
    "        # composites\n",
    "        matmap_np.update({\n",
    "            \"ZZ\": np.kron(matmap_np['Z'], matmap_np['Z']),\n",
    "            \"IX\": np.kron(matmap_np['I'], matmap_np['X']),\n",
    "            \"XI\": np.kron(matmap_np['X'], matmap_np['I']),\n",
    "            \"YY\": np.array([[ 0,  0,  0, -1],  # to avoid complex numbers\n",
    "                            [ 0,  0,  1,  0],\n",
    "                            [ 0,  1,  0,  0],\n",
    "                            [-1,  0,  0,  0]], dtype=dtype)\n",
    "        })\n",
    "        for i in range(2, 11):\n",
    "            matmap_np[\"I\"*i] = np.eye(2**i, dtype=dtype)\n",
    "        # add 'Y' only if dtype supports imaginary numbers\n",
    "        if np.issubdtype(dtype, np.complexfloating):\n",
    "            matmap_np[\"Y\"] = np.array([[0, -1j], [1j, 0]], dtype=dtype)\n",
    "\n",
    "        # sparse versions\n",
    "        matmap_sp = {k: sp.csr_array(v) for k, v in matmap_np.items()}\n",
    "    \n",
    "    if not np.issubdtype(dtype, np.complexfloating) and \"Y\" in hamiltonian:\n",
    "        raise ValueError(f\"The Pauli matrix Y is not supported for dtype {dtype.__name__}.\")\n",
    "\n",
    "    matmap = matmap_sp if sparse else matmap_np\n",
    "\n",
    "    # only use buffer if pre-computed chunks are available or if new chunks are allowed to be stored\n",
    "    use_buffer = buffer is None or len(buffer) > 0 or max_buffer_n > 0\n",
    "    if use_buffer and buffer is None:\n",
    "        buffer = matmap\n",
    "\n",
    "    def calculate_chunk_matrix(chunk, sparse=False, scaling=1):\n",
    "        # if scaling != 1:  # only relevant for int dtype\n",
    "            # scaling = np.array(scaling, dtype=dtype)\n",
    "        if use_buffer:\n",
    "            if chunk in buffer:\n",
    "                return buffer[chunk] if scaling == 1 else scaling * buffer[chunk]\n",
    "            if len(chunk) == 1:\n",
    "                return matmap[chunk[0]] if scaling == 1 else scaling * matmap[chunk[0]]\n",
    "            # Check if a part of the chunk has already been calculated\n",
    "            for i in range(len(chunk)-1, 1, -1):\n",
    "                for j in range(len(chunk)-i+1):\n",
    "                    subchunk = chunk[j:j+i]\n",
    "                    if subchunk in buffer:\n",
    "                        # If so, calculate the rest of the chunk recursively\n",
    "                        parts = [chunk[:j], subchunk, chunk[j+i:]]\n",
    "                        # remove empty chunks\n",
    "                        parts = [c for c in parts if c != \"\"]\n",
    "                        # See where to apply the scaling\n",
    "                        shortest = min(parts, key=len)\n",
    "                        # Calculate each part recursively\n",
    "                        for i, c in enumerate(parts):\n",
    "                            if c == subchunk:\n",
    "                                if c == shortest:\n",
    "                                    parts[i] = scaling * buffer[c]\n",
    "                                    shortest = \"\"\n",
    "                                else:\n",
    "                                    parts[i] = buffer[c]\n",
    "                            else:\n",
    "                                if c == shortest:\n",
    "                                    parts[i] = calculate_chunk_matrix(c, sparse=sparse, scaling=scaling)\n",
    "                                    shortest = \"\"\n",
    "                                else:\n",
    "                                    parts[i] = calculate_chunk_matrix(c, sparse=sparse, scaling=1)\n",
    "                        return reduce(kron, parts)\n",
    "\n",
    "        # Calculate the chunk matrix gate by gate\n",
    "        if use_buffer and len(chunk) <= max_buffer_n:\n",
    "            gates = [matmap[gate] for gate in chunk]\n",
    "            chunk_matrix = reduce(kron, gates)\n",
    "            buffer[chunk] = chunk_matrix\n",
    "            if scaling != 1:\n",
    "                chunk_matrix = scaling * chunk_matrix\n",
    "        else:\n",
    "            gates = [scaling * matmap[chunk[0]]] + [matmap[gate] for gate in chunk[1:]]\n",
    "            chunk_matrix = reduce(kron, gates)\n",
    "\n",
    "        return chunk_matrix\n",
    "\n",
    "    # Remove whitespace\n",
    "    hamiltonian = hamiltonian.replace(\" \", \"\")\n",
    "    # replace - with +-, except before e\n",
    "    hamiltonian = hamiltonian \\\n",
    "                    .replace(\"-\", \"+-\") \\\n",
    "                    .replace(\"e+-\", \"e-\") \\\n",
    "                    .replace(\"(+-\", \"(-\")\n",
    "\n",
    "    # print(\"parse_hamiltonian: Pre-processed Hamiltonian:\", hamiltonian)\n",
    "\n",
    "    # Find parts in parentheses\n",
    "    part = \"\"\n",
    "    parts = []\n",
    "    depth = 0\n",
    "    current_part_weight = \"\"\n",
    "    for i, c in enumerate(hamiltonian):\n",
    "        if c == \"(\":\n",
    "            if depth == 0:\n",
    "                # for top-level parts search backwards for the weight\n",
    "                weight = \"\"\n",
    "                for j in range(i-1, -1, -1):\n",
    "                    if hamiltonian[j] in [\"(\"]:\n",
    "                        break\n",
    "                    weight += hamiltonian[j]\n",
    "                    if hamiltonian[j] in [\"+\", \"-\"]:\n",
    "                        break\n",
    "                weight = weight[::-1]\n",
    "                if weight != \"\":\n",
    "                    current_part_weight = weight\n",
    "            depth += 1\n",
    "        elif c == \")\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                part += c\n",
    "                parts.append((current_part_weight, part))\n",
    "                part = \"\"\n",
    "                current_part_weight = \"\"\n",
    "        if depth > 0: \n",
    "            part += c\n",
    "\n",
    "    # print(\"Parts found:\", parts)\n",
    "\n",
    "    # Replace parts in parentheses with a placeholder\n",
    "    for i, (weight, part) in enumerate(parts):\n",
    "        hamiltonian = hamiltonian.replace(weight+part, f\"+part{i}\", 1)\n",
    "        # remove * at the end of the weight\n",
    "        if weight != \"\" and weight[-1] == \"*\":\n",
    "            weight = weight[:-1]\n",
    "        if weight in [\"\", \"+\", \"-\"]:\n",
    "            weight += \"1\"\n",
    "        # Calculate the part recursively\n",
    "        part = part[1:-1] # remove parentheses\n",
    "        parts[i] = parse_hamiltonian(part, sparse=sparse, scaling=float(weight), buffer=buffer, max_buffer_n=max_buffer_n, dtype=dtype)\n",
    "\n",
    "    # print(\"Parts replaced:\", parts)\n",
    "\n",
    "    # Parse the rest of the Hamiltonian\n",
    "    chunks = hamiltonian.split(\"+\")\n",
    "    # Remove empty chunks\n",
    "    chunks = [c for c in chunks if c != \"\"]\n",
    "    # If parts are present, use them to determine the number of qubits\n",
    "    if parts:\n",
    "        n = int(np.log2(parts[0].shape[0]))\n",
    "    else: # Use chunks to determine the number of qubits\n",
    "        n = 0\n",
    "        for c in chunks:\n",
    "            if c[0] in [\"-\", \"+\"]:\n",
    "                c = c[1:]\n",
    "            if \"*\" in c:\n",
    "                c = c.split(\"*\")[1]\n",
    "            if c.startswith(\"part\"):\n",
    "                continue\n",
    "            try:\n",
    "                float(c)\n",
    "                continue\n",
    "            except ValueError:\n",
    "                n = len(c)\n",
    "                break\n",
    "        if n == 0:\n",
    "            print(\"Warning: Hamiltonian is a scalar!\")\n",
    "\n",
    "    if not sparse and n > 10:\n",
    "        # check if we would blow up the memory\n",
    "        mem_required = 2**(2*n) * np.array(1, dtype=dtype).nbytes\n",
    "        mem_available = psutil.virtual_memory().available\n",
    "        if mem_required > mem_available:\n",
    "            raise MemoryError(f\"This would blow up you memory ({duh(mem_required)} required)! Try using `sparse=True`.\")\n",
    "\n",
    "    if sparse:\n",
    "        H = sp.csr_array((2**n, 2**n), dtype=dtype)\n",
    "    else:\n",
    "        if n > 10:\n",
    "            print(f\"Warning: Using a dense matrix for a {n}-qubit Hamiltonian is not recommended. Use sparse=True.\")\n",
    "        H = np.zeros((2**n, 2**n), dtype=dtype)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # print(\"Processing chunk:\", chunk)\n",
    "        chunk_matrix = None\n",
    "        if chunk == \"\":\n",
    "            continue\n",
    "        # Parse the weight of the chunk\n",
    "        \n",
    "        if chunk.startswith(\"part\"):\n",
    "            weight = 1  # parts are already scaled\n",
    "            chunk_matrix = parts[int(chunk.split(\"part\")[1])]\n",
    "        elif \"*\" in chunk:\n",
    "            weight = float(chunk.split(\"*\")[0])\n",
    "            chunk = chunk.split(\"*\")[1]\n",
    "        elif len(chunk) == n+1 and chunk[0] in [\"-\", \"+\"] and n >= 1 and chunk[1] in matmap:\n",
    "            weight = float(chunk[0] + \"1\")\n",
    "            chunk = chunk[1:]\n",
    "        elif (chunk[0] in [\"-\", \"+\", \".\"] or chunk[0].isdigit()) and all([c not in matmap for c in chunk[1:]]):\n",
    "            if len(chunk) == 1 and chunk[0] in [\"-\", \".\"]:\n",
    "                chunk = 0\n",
    "            weight = complex(chunk)\n",
    "            if np.iscomplex(weight):\n",
    "                raise ValueError(\"Complex scalars would make the Hamiltonian non-Hermitian!\")\n",
    "            weight = weight.real\n",
    "            # weight = np.array(weight, dtype=dtype)  # only relevant for int dtype\n",
    "            chunk_matrix = np.eye(2**n, dtype=dtype)\n",
    "        elif len(chunk) != n:\n",
    "            raise ValueError(f\"Gate count must be {n} but was {len(chunk)} for chunk \\\"{chunk}\\\"\")\n",
    "        else:\n",
    "            weight = 1\n",
    "\n",
    "        if chunk_matrix is None:\n",
    "            chunk_matrix = calculate_chunk_matrix(chunk, sparse=sparse, scaling = scaling * weight)\n",
    "        elif scaling * weight != 1:\n",
    "            chunk_matrix = scaling * weight * chunk_matrix\n",
    "\n",
    "        # Add the chunk to the Hamiltonian\n",
    "        # print(\"Adding chunk\", weight, chunk, \"for hamiltonian\", scaling, hamiltonian)\n",
    "        # print(type(H), H.dtype, type(chunk_matrix), chunk_matrix.dtype)\n",
    "        if len(chunks) == 1:\n",
    "            H = chunk_matrix\n",
    "        else:\n",
    "            H += chunk_matrix\n",
    "\n",
    "    if sparse:\n",
    "        assert np.allclose(H.data, H.conj().T.data), f\"The given Hamiltonian {hamiltonian} is not Hermitian: {H.data}\"\n",
    "    else:\n",
    "        assert np.allclose(H, H.conj().T), f\"The given Hamiltonian {hamiltonian} is not Hermitian: {H}\"\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_point.hamiltonian)\n",
    "hamiltonian = parse_hamiltonian(data_point.hamiltonian)\n",
    "print(hamiltonian.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dims(data_point):\n",
    "    dims = data_point.grid_extent\n",
    "\n",
    "    assert len(dims) <= 2\n",
    "    \n",
    "    if len(data_point.grid_extent) == 1:\n",
    "        if dims[0] == 0:\n",
    "            dims[0] = len(data_point.x_nodes)\n",
    "        dims = np.append(dims, 1)\n",
    "\n",
    "    assert np.prod(dims) == len(data_point.x_nodes)\n",
    "    return dims\n",
    "\n",
    "dims = get_dims(data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rectangular_peps_pbc(height: int, width: int):\n",
    "    \"\"\"\n",
    "    Creates a structure matrix of a rectangular lattice tensor network with periodic boundary\n",
    "    conditions (pbc) of shape (height x width). The total number of tensors in the network would be height x width.\n",
    "    :param height: The height of the tensor network.\n",
    "    :param width: The width of the tensor network.\n",
    "    :return: a structure matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # create tuples of tensor indices\n",
    "    edge_list = []\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            if height > 1:\n",
    "                i_down = (i + 1) % height\n",
    "                edge_list.append((i, j, 4, i_down, j, 2))\n",
    "            \n",
    "            if width > 1:\n",
    "                j_right = (j + 1) % width\n",
    "                edge_list.append((i, j, 3, i, j_right, 1))\n",
    "    \n",
    "    structure_matrix = np.zeros(shape=[height * width, len(edge_list)], dtype=int)\n",
    "    # fill in the structure matrix\n",
    "    for edge_idx, edge in enumerate(edge_list):\n",
    "        node_a_idx = np.ravel_multi_index([edge[0], edge[1]], (height, width))\n",
    "        node_b_idx = np.ravel_multi_index([edge[3], edge[4]], (height, width))\n",
    "\n",
    "        structure_matrix[node_a_idx, edge_idx] = edge[2]\n",
    "        structure_matrix[node_b_idx, edge_idx] = edge[5]\n",
    "\n",
    "    # reorder dimension according to a constant order\n",
    "    for i in range(structure_matrix.shape[0]):\n",
    "        row = structure_matrix[i, np.nonzero(structure_matrix[i, :])[0]]\n",
    "        new_row = np.array(range(1, len(row) + 1))\n",
    "        order = np.argsort(row)\n",
    "        new_row = new_row[order]\n",
    "        structure_matrix[i, np.nonzero(structure_matrix[i, :])[0]] = new_row\n",
    "\n",
    "    return structure_matrix\n",
    "\n",
    "\n",
    "\n",
    "def rectangular_peps_obc(height: int, width: int):\n",
    "    \"\"\"\n",
    "    Creates a structure matrix of a rectangular lattice tensor network with open (non-periodic) boundary\n",
    "    conditions (obc) of shape (height x width). The total number of tensors in the network would be height x width.\n",
    "    :param height: The height of the tensor network.\n",
    "    :param width: The width of the tensor network.\n",
    "    :return: a structure matrix\n",
    "    \"\"\"\n",
    "    # edge = (node_a i, node_a j, node_a l, node_b i, node_b j, node_b l)\n",
    "\n",
    "    # create tuples of tensor indices\n",
    "    edge_list = []\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            if i < height - 1:\n",
    "                edge_list.append((i, j, 4, i + 1, j, 2))\n",
    "            if j < width - 1:\n",
    "                edge_list.append((i, j, 3, i, j + 1, 1))\n",
    "    structure_matrix = np.zeros(shape=[height * width, len(edge_list)], dtype=int)\n",
    "\n",
    "    # fill in the structure matrix\n",
    "    for edge_idx, edge in enumerate(edge_list):\n",
    "        node_a_idx = np.ravel_multi_index([edge[0], edge[1]], (height, width))\n",
    "        node_b_idx = np.ravel_multi_index([edge[3], edge[4]], (height, width))\n",
    "\n",
    "        structure_matrix[node_a_idx, edge_idx] = edge[2]\n",
    "        structure_matrix[node_b_idx, edge_idx] = edge[5]\n",
    "\n",
    "    # reorder dimension according to a constant order\n",
    "    for i in range(structure_matrix.shape[0]):\n",
    "        row = structure_matrix[i, np.nonzero(structure_matrix[i, :])[0]]\n",
    "        new_row = np.array(range(1, len(row) + 1))\n",
    "        order = np.argsort(row)\n",
    "        new_row = new_row[order]\n",
    "        structure_matrix[i, np.nonzero(structure_matrix[i, :])[0]] = new_row\n",
    "    return structure_matrix\n",
    "\n",
    "\n",
    "\n",
    "#This generates Rectangular PEPS with OBC or PBC. Structure Matrix defined in https://arxiv.org/pdf/1808.00680.pdf\n",
    "def get_structure_matrix(data_point, circular = False):\n",
    "    \"\"\" Returns the structure matrix from a given data point. \"\"\"\n",
    "    dims = get_dims(data_point)\n",
    "    if circular:\n",
    "        smat = rectangular_peps_pbc(dims[0], dims[1])\n",
    "    else:\n",
    "        smat = rectangular_peps_obc(dims[0], dims[1])\n",
    "    return smat\n",
    "\n",
    "\n",
    "smat = get_structure_matrix(data_point, circular=True)\n",
    "print(smat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tnsu.tensor_network as tn\n",
    "from simple_update import SimpleUpdate\n",
    "\n",
    "\n",
    "tensornet = tn.TensorNetwork(structure_matrix=smat, virtual_dim=2, spin_dim=2)\n",
    "\n",
    "\n",
    "# pauli matrices\n",
    "pauli_x = np.array([[0, 1],\n",
    "                    [1, 0]])\n",
    "pauli_z = np.array([[1., 0],\n",
    "                    [0, -1]])\n",
    "\n",
    "# ITE time constants\n",
    "dts = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "# Local spin operators\n",
    "s_i = [pauli_x]\n",
    "s_j = [pauli_z]\n",
    "\n",
    "# The field-spin operators \n",
    "s_k = [pauli_z]\n",
    "\n",
    "# The maximal virtual bond dimension (used for SU truncation)\n",
    "d_max = 5\n",
    "\n",
    "# The Hamiltonian's 2-body interaction constants \n",
    "j_ij = data_point.x_edges.squeeze().numpy()\n",
    "\n",
    "# The Hamiltonian's 1-body field constant\n",
    "h_k = data_point.x_nodes.squeeze().numpy()\n",
    "\n",
    "USE_HAMILTONIAN = False\n",
    "\n",
    "\n",
    "\n",
    "su = SimpleUpdate(tensor_network=tensornet, \n",
    "                          dts=dts, \n",
    "                          j_ij=j_ij, \n",
    "                          h_k=h_k, \n",
    "                          s_i=s_i, \n",
    "                          s_j=s_j, \n",
    "                          s_k=s_k,\n",
    "                          hamiltonian=hamiltonian if USE_HAMILTONIAN else None, \n",
    "                          d_max=d_max, \n",
    "                          max_iterations=200, \n",
    "                          convergence_error=1e-6, \n",
    "                          log_energy=True,\n",
    "                          print_process=True)\n",
    "\n",
    "\n",
    "su.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain Labels\n",
    "y_energy = data_point.y_energy\n",
    "one_rdms = data_point.y_node_rdms\n",
    "two_rdms = data_point.y_edge_rdms\n",
    "\n",
    "#Obtain Predictions\n",
    "pred_energy = su.energy_per_site()\n",
    "print(\"Calculated Energy: \", pred_energy)\n",
    "print(\"True Energy: \", y_energy.item())\n",
    "\n",
    "\n",
    "#Compare the first one-body RDM\n",
    "pred_one_rdm = su.tensor_rdm(tensor_index=0)\n",
    "print(\"Calculated One RDM: \", pred_one_rdm)\n",
    "print(\"True One RDM: \", one_rdms[0])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
